{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45gx2t7G764t",
        "outputId": "aa44c456-b812-48cd-f90f-5ad740af5764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "#Mounting Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nvD3kfA81p1",
        "outputId": "08d54f97-90ba-4855-ed59-d00ab162ccda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib-venn\n",
            "  Downloading matplotlib-venn-1.1.2.tar.gz (40 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/40.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (1.16.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->matplotlib-venn) (1.17.0)\n",
            "Building wheels for collected packages: matplotlib-venn\n",
            "  Building wheel for matplotlib-venn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for matplotlib-venn: filename=matplotlib_venn-1.1.2-py3-none-any.whl size=45388 sha256=13fe4ee680a9d69267f1ebedc1731417968c214a5ad06ce445c10404a360fa69\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/47/0c/f014c55a1cfd56dce41a1cafd23e3c590652b5e71330cc181c\n",
            "Successfully built matplotlib-venn\n",
            "Installing collected packages: matplotlib-venn\n",
            "Successfully installed matplotlib-venn-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "!pip install matplotlib-venn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAtV5dwO9nyF"
      },
      "outputs": [],
      "source": [
        "import condacolab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx3ZKXEU-oc6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn.pipeline import Pipeline\n",
        "from collections import Counter\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from matplotlib_venn import venn3\n",
        "from functools import reduce\n",
        "import shutil\n",
        "import sys\n",
        "import os, subprocess, shlex\n",
        "# Parallel Processing with Multiprocessing\n",
        "import gzip\n",
        "from multiprocessing import Pool, cpu_count\n",
        "# from Bio import SeqIO\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
        "import shutil\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import multiprocessing as mp\n",
        "import glob, math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shlex"
      ],
      "metadata": {
        "id": "C_F7Ikg-7Oun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CzATe5q84dj",
        "outputId": "4a226137-885f-4bc2-9359-8046ad0db27f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "ðŸ“¦ Installing...\n",
            "ðŸ“Œ Adjusting configuration...\n",
            "ðŸ©¹ Patching environment...\n",
            "â² Done in 0:00:07\n",
            "ðŸ” Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvPkrt0K9VbY",
        "outputId": "99883bb8-bef2-442a-e443-0e01cf75c316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ¨ðŸ°âœ¨ Everything looks OK!\n"
          ]
        }
      ],
      "source": [
        "condacolab.check()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JEiDLH80OQI"
      },
      "outputs": [],
      "source": [
        "conda_path = '/usr/local/bin'\n",
        "os.environ['CONDA_EXE'] = os.path.join(conda_path, 'conda')\n",
        "os.environ['PATH'] = f\"{conda_path}:{os.environ['PATH']}\"\n",
        "sys.path.insert(0, os.path.join(conda_path, '..', 'lib', 'python3.10', 'site-packages'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwSCeT4TKAcE",
        "outputId": "d0affd46-a98b-483e-967b-962a5a70f164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: cd: usr/local/bin: No such file or directory\n",
            "--2025-09-01 02:15:36--  https://github.com/milaboratory/mixcr/releases/download/v4.6.0/mixcr-4.6.0.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/33134597/6bdd7a80-9aea-4572-b796-6065ef5d1f8b?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-01T02%3A59%3A11Z&rscd=attachment%3B+filename%3Dmixcr-4.6.0.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-01T01%3A58%3A18Z&ske=2025-09-01T02%3A59%3A11Z&sks=b&skv=2018-11-09&sig=R8nshWEMSwVU7XO5fasSww410T0MSH%2FKGkq7VM4cR6Y%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NjY5MzIzNywibmJmIjoxNzU2NjkyOTM3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.3ZJKmK0wARpCEUV_DUMf-eWkvNh_rLsCzaeJhD392dU&response-content-disposition=attachment%3B%20filename%3Dmixcr-4.6.0.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-09-01 02:15:37--  https://release-assets.githubusercontent.com/github-production-release-asset/33134597/6bdd7a80-9aea-4572-b796-6065ef5d1f8b?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-01T02%3A59%3A11Z&rscd=attachment%3B+filename%3Dmixcr-4.6.0.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-01T01%3A58%3A18Z&ske=2025-09-01T02%3A59%3A11Z&sks=b&skv=2018-11-09&sig=R8nshWEMSwVU7XO5fasSww410T0MSH%2FKGkq7VM4cR6Y%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NjY5MzIzNywibmJmIjoxNzU2NjkyOTM3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.3ZJKmK0wARpCEUV_DUMf-eWkvNh_rLsCzaeJhD392dU&response-content-disposition=attachment%3B%20filename%3Dmixcr-4.6.0.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 104617347 (100M) [application/octet-stream]\n",
            "Saving to: â€˜mixcr-4.6.0.zipâ€™\n",
            "\n",
            "mixcr-4.6.0.zip     100%[===================>]  99.77M   102MB/s    in 1.0s    \n",
            "\n",
            "2025-09-01 02:15:38 (102 MB/s) - â€˜mixcr-4.6.0.zipâ€™ saved [104617347/104617347]\n",
            "\n",
            "/bin/bash: line 1: cd: too many arguments\n"
          ]
        }
      ],
      "source": [
        "# Installation Steps for MiXCR\n",
        "%cd /\n",
        "%cd /usr/local/bin\n",
        "!wget https://github.com/milaboratory/mixcr/releases/download/v4.6.0/mixcr-4.6.0.zip\n",
        "!unzip mixcr-4.6.0.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZL_IQr-LVur"
      },
      "outputs": [],
      "source": [
        "# Setting up license key for MiXCR\n",
        "os.environ['MI_LICENSE'] = 'E-UZCNDGRBYRVXJUSGMPFWBGRMAGYVBGJHUSXMTPRSHWVCSOGL'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_dIoXBLKVZk",
        "outputId": "ad0da712-c3bc-4e7e-e36d-f8cb282ebb86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: \u001b[1mmixcr\u001b[21m\u001b[0m [\u001b[33m-h\u001b[39m\u001b[0m] [\u001b[33m-v\u001b[39m\u001b[0m] [COMMAND]\n",
            "  \u001b[33m-h\u001b[39m\u001b[0m, \u001b[33m--help\u001b[39m\u001b[0m      Show this help message and exit.\n",
            "  \u001b[33m-v\u001b[39m\u001b[0m, \u001b[33m--version\u001b[39m\u001b[0m   Print version information and exit\n",
            "Base commands:\n",
            "  \u001b[1manalyze\u001b[21m\u001b[0m            Run full MiXCR pipeline for specific input.\n",
            "  \u001b[1malign\u001b[21m\u001b[0m              Builds alignments with V,D,J and C genes for input sequencing reads.\n",
            "  \u001b[1mrefineTagsAndSort\u001b[21m\u001b[0m  Applies error correction algorithm for tag sequences and sorts resulting file\n",
            "                       by tags.\n",
            "  \u001b[1massemblePartial\u001b[21m\u001b[0m    Assembles partially aligned reads into longer sequences.\n",
            "  \u001b[1mextend\u001b[21m\u001b[0m             Impute alignments or clones with germline sequences.\n",
            "  \u001b[1massemble\u001b[21m\u001b[0m           Assemble clones.\n",
            "  \u001b[1massembleContigs\u001b[21m\u001b[0m    Assemble full sequences.\n",
            "  \u001b[1mgroupClones\u001b[21m\u001b[0m        Group clones by cells. Required data with cell tags.\n",
            "  \u001b[1mfindAlleles\u001b[21m\u001b[0m        Find allele variants in clnx.\n",
            "  \u001b[1mfindShmTrees\u001b[21m\u001b[0m       Builds SHM trees.\n",
            "  \u001b[1mdownsample\u001b[21m\u001b[0m         Downsample clonesets.\n",
            "  \u001b[1mqc\u001b[21m\u001b[0m                 Perform quality control checks on results.\n",
            "Postanalysis commands:\n",
            "  \u001b[1mpostanalysis\u001b[21m\u001b[0m  Run postanalysis routines. This command has subcommands, use -h to see more\n",
            "Export commands:\n",
            "  \u001b[1mexportTables\u001b[21m\u001b[0m              CDR3 metrics, Diversity, V/J/VJ-Usage, CDR3/V-Spectratype, Overlap\n",
            "  \u001b[1mexportPreprocTables\u001b[21m\u001b[0m       Export preprocessing summary tables.\n",
            "  \u001b[1mexportPlots\u001b[21m\u001b[0m               Export postanalysis plots. This command has subcommands, use -h to see\n",
            "                              more\n",
            "  \u001b[1moverlapScatterPlot\u001b[21m\u001b[0m        Plot overlap scatter-plot.\n",
            "  \u001b[1mexportAlignments\u001b[21m\u001b[0m          Export V/D/J/C alignments into tab delimited file.\n",
            "  \u001b[1mexportAlignmentsPretty\u001b[21m\u001b[0m    Export verbose information about alignments.\n",
            "  \u001b[1mexportClones\u001b[21m\u001b[0m              Export assembled clones into tab delimited file.\n",
            "  \u001b[1mexportCloneGroups\u001b[21m\u001b[0m         Export clone groups into tab delimited file. Data should be processed\n",
            "                              by `groupClones`\n",
            "  \u001b[1mexportClonesPretty\u001b[21m\u001b[0m        Export verbose information about clones.\n",
            "  \u001b[1mexportShmTreesWithNodes\u001b[21m\u001b[0m   Export SHMTree as a table with a row for every clone or reconstructed\n",
            "                              node for each root (only one root if no single cell data)\n",
            "  \u001b[1mexportShmTrees\u001b[21m\u001b[0m            Export SHMTree as a table with a row for every SHM root in a table\n",
            "                              (single row if no single cell data)\n",
            "  \u001b[1mexportShmSingleCellTrees\u001b[21m\u001b[0m  Export SHM trees with one row per node. Tree may contain several roots,\n",
            "                              that will be exported in separate columns. Initial data for building\n",
            "                              tree should contain cell data.\n",
            "  \u001b[1mexportShmTreesNewick\u001b[21m\u001b[0m      Export SHMTree as newick\n",
            "  \u001b[1mexportReports\u001b[21m\u001b[0m             Export MiXCR reports.\n",
            "  \u001b[1mexportReportsTable\u001b[21m\u001b[0m        Export reports from files in tabular format.\n",
            "  \u001b[1mexportQc\u001b[21m\u001b[0m                  Export QC plots. This command has subcommands, use -h to see more\n",
            "  \u001b[1mexportClonesOverlap\u001b[21m\u001b[0m       Build cloneset overlap and export into tab delimited file.\n",
            "  \u001b[1mexportAirr\u001b[21m\u001b[0m                Exports a clns, clna or vdjca file to Airr formatted tsv file.\n",
            "Util commands:\n",
            "  \u001b[1mexportReadsForClones\u001b[21m\u001b[0m       Export reads for particular clones from \"clones & alignments\" (*.clna)\n",
            "                               file.\n",
            "  \u001b[1mexportAlignmentsForClones\u001b[21m\u001b[0m  Export alignments for particular clones from \"clones & alignments\" (*.\n",
            "                               clna) file.\n",
            "  \u001b[1mexportReads\u001b[21m\u001b[0m                Export original reads from vdjca file.\n",
            "  \u001b[1mmergeAlignments\u001b[21m\u001b[0m            Merge several *.vdjca files with alignments into a single alignments\n",
            "                               file.\n",
            "  \u001b[1mfilterAlignments\u001b[21m\u001b[0m           Filter alignments.\n",
            "  \u001b[1msortAlignments\u001b[21m\u001b[0m             Sort alignments in vdjca file by read id.\n",
            "  \u001b[1msortClones\u001b[21m\u001b[0m                 Sort clones by sequence. Clones in the output file will be sorted by\n",
            "                               clonal sequence, which allows to build overlaps between clonesets.\n",
            "  \u001b[1malignmentsDiff\u001b[21m\u001b[0m             Calculates the difference between two .vdjca files.\n",
            "  \u001b[1mclonesDiff\u001b[21m\u001b[0m                 Calculates the difference between two .clns files.\n",
            "  \u001b[1mversionInfo\u001b[21m\u001b[0m                Output information about MiXCR version which generated the file.\n",
            "  \u001b[1mslice\u001b[21m\u001b[0m                      Slice vdjca|clns|clna|shmt file.\n",
            "  \u001b[1mexportPreset\u001b[21m\u001b[0m               Export a preset file given the preset name or source file and a set of\n",
            "                               mix-ins\n",
            "  \u001b[1mbuildLibrary\u001b[21m\u001b[0m               Build custom reference library\n",
            "  \u001b[1mmergeLibrary\u001b[21m\u001b[0m               Merge multiple reference libraries in one file\n",
            "  \u001b[1mdebugLibrary\u001b[21m\u001b[0m               Debug reference library file\n",
            "  \u001b[1mlistPresets\u001b[21m\u001b[0m                Show all available presets\n"
          ]
        }
      ],
      "source": [
        "!mixcr -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u28mUNXNM_bj",
        "outputId": "b073c71a-dcd8-4720-f957-94cb078c1659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/ cd usr/local/bin | git clone https://github.com/liulab-dfci/TRUST4.git'\n",
            "/content\n",
            "/bin/bash: line 1: cd: usr/local/bin/TRUST4: No such file or directory\n",
            "make: *** No targets specified and no makefile found.  Stop.\n"
          ]
        }
      ],
      "source": [
        "# Steps to install trust4\n",
        "%cd /\n",
        "%cd /usr/local/bin\n",
        "!git clone https://github.com/liulab-dfci/TRUST4.git\n",
        "%cd /usr/local/bin/TRUST4\n",
        "!make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPiv21O9N4zz",
        "outputId": "f08bd748-fe2a-46a1-db93-4e8ef0688bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRUST4 path added to PATH: /usr/local/bin/TRUST4:/usr/local/bin:/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ],
      "source": [
        "# Add TRUST4 to the system's PATH\n",
        "trust4_path = \"/usr/local/bin/TRUST4\"\n",
        "os.environ['PATH'] = f\"{trust4_path}:{os.environ['PATH']}\"\n",
        "print(f\"TRUST4 path added to PATH: {os.environ['PATH']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxJdQ-LNOUXj",
        "outputId": "f26440fa-1e28-4c38-d8a9-0acc4615e0a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRUST4 v1.1.7-r606 usage: ./run-trust4 [OPTIONS]:\n",
            "Required:\n",
            "\t-b STRING: path to bam file\n",
            "\t-1 STRING -2 STRING: path to paired-end read files\n",
            "\t-u STRING: path to single-end read file\n",
            "\t-f STRING: path to the fasta file coordinate and sequence of V/D/J/C genes\n",
            "Optional:\n",
            "\t--ref STRING: path to detailed V/D/J/C gene reference file from IMGT database. (default: not used but recommended)\n",
            "\t-o STRING: prefix of output files. (default: inferred from file prefix)\n",
            "\t--od STRING: the directory for output files. (default: ./)\n",
            "\t-t INT: number of threads (default: 1)\n",
            "\t-k INT: the starting k-mer size for indexing contigs (default: 9)\n",
            "\t--barcode STRING: if -b, bam field for barcode; if -1 -2/-u, file containing barcodes (default: not used)\n",
            "\t--barcodeLevel STRING: barcode is for cell or molecule (default: cell)\n",
            "\t--barcodeWhitelist STRING: path to the barcode whitelist (default: not used)\n",
            "\t--barcodeTranslate STRING: path to the barcode translate file (default: not used)\n",
            "\t--UMI STRING: if -b, bam field for 10x Genomics-like UMI; if -1 -2/-u, file containing 10x Genomics-like UMIs (default: not used)\n",
            "\t--readFormat STRING: format for read, barcode and UMI files (example: r1:0:-1,r2:0:-1,bc:0:15,um:16:-1 for paired-end files with barcode and UMI)\n",
            "\t--repseq: the data is from bulk,non-UMI-based TCR-seq or BCR-seq (default: not set)\n",
            "\t--contigMinCov INT: ignore contigs that have bases covered by fewer than INT reads (default: 0)\n",
            "\t--minHitLen INT: the minimal hit length for a valid overlap (default: auto)\n",
            "\t--mateIdSuffixLen INT: the suffix length in read id for mate. (default: not used)\n",
            "\t--skipMateExtension: do not extend assemblies with mate information, useful for SMART-seq (default: not used)\n",
            "\t--skipReadRealign: do not realign reads in annotator, useful for reducing computation cost of barcode/UMI-based repseq (default: not used)\n",
            "\t--abnormalUnmapFlag: the flag in BAM for the unmapped read-pair is nonconcordant (default: not set)\n",
            "\t--noExtraction: directly use the files from provided -1 -2/-u to assemble (default: extraction first)\n",
            "\t--imgtAdditionalGap STRING: description for additional gap codon position in IMGT (0-based), e.g. \"TRAV:7,83\" for mouse (default: no)\n",
            "\t--assembleWithRef: conduct the assembly with --ref file (default: use -f file)\n",
            "\t--outputReadAssignment: output read assignment results to the prefix_assign.out file (default: no output)\n",
            "\t--stage INT: start TRUST4 on specified stage (default: 0):\n",
            "\t\t0: start from beginning (candidate read extraction)\n",
            "\t\t1: start from assembly\n",
            "\t\t2: start from annotation\n",
            "\t\t3: start from generating the report table\n",
            "\t--clean INT: clean up files. 0: no clean. 1: clean intermediate files. 2: only keep AIRR files. (default: 0)\n"
          ]
        }
      ],
      "source": [
        "!run-trust4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35A6JKWn-iql",
        "outputId": "2d79c8e7-edfa-410d-9364-fd5b9586bd85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Looking for: ['sra-tools']\n",
            "\n",
            "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[0Gbioconda/linux-64 (check zst)                      Checked  0.1s\n",
            "\u001b[?25h\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "bioconda/noarch (c..  â£¾  \u001b[2K\u001b[1A\u001b[2K\u001b[0Gbioconda/noarch (check zst)                       \n",
            "\u001b[?25h\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "bioconda/linux-64  â£¾  \n",
            "bioconda/noarch    â£¾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
            "conda-forge/linux-64  â£¾  \n",
            "conda-forge/noarch    â£¾  \n",
            "bioconda/linux-64      1%\n",
            "bioconda/noarch       â£¾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\n",
            "conda-forge/linux-64   4%\n",
            "conda-forge/noarch     5%\n",
            "bioconda/linux-64     45%\n",
            "bioconda/noarch       28%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gbioconda/linux-64                                 \n",
            "[+] 0.3s\n",
            "conda-forge/linux-64  11%\n",
            "conda-forge/noarch    19%\n",
            "bioconda/noarch       87%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gbioconda/noarch                                   \n",
            "[+] 0.4s\n",
            "conda-forge/linux-64  21%\n",
            "conda-forge/noarch    39%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\n",
            "conda-forge/linux-64  33%\n",
            "conda-forge/noarch    65%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\n",
            "conda-forge/linux-64  45%\n",
            "conda-forge/noarch    91%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.7s\n",
            "conda-forge/linux-64  45%\n",
            "conda-forge/noarch    91%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/noarch                                \n",
            "[+] 0.8s\n",
            "conda-forge/linux-64  61%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\n",
            "conda-forge/linux-64  82%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\n",
            "conda-forge/linux-64  93%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\n",
            "conda-forge/linux-64  93%\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/linux-64                              \n",
            "\u001b[?25hYour pinning does not match what's currently installed. Please remove the pin and fix your installation\n",
            "  Pin: python=3.12\n",
            "  Currently installed: conda-forge/linux-64::python==3.11.11=h9e4cc4f_1_cpython\n"
          ]
        }
      ],
      "source": [
        "!mamba install -c bioconda sra-tools -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZxsD5E6phya"
      },
      "outputs": [],
      "source": [
        "!mamba create -n tcr-env -c bioconda -c conda-forge -c mamba-forge -c defaults python=3.11 trust4 mixcr biopython -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU8hZS5dBGPZ",
        "outputId": "84e0c985-27e8-4f05-bc2a-2ddeaa9dea77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient1 FASTQs already exist â†’ skipping download.\n",
            "Patient2 FASTQs already exist â†’ skipping download.\n",
            "Patient3 FASTQs already exist â†’ skipping download.\n"
          ]
        }
      ],
      "source": [
        "p1 = \"/content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient1\"\n",
        "p2 = \"/content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient2\"\n",
        "p3 = \"/content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient3\"\n",
        "\n",
        "os.makedirs(p2, exist_ok=True)\n",
        "os.makedirs(p3, exist_ok=True)\n",
        "\n",
        "# Patient1\n",
        "if not any(os.scandir(p1)):  # empty dir\n",
        "    print(\"Patient1 FASTQs not found â†’ downloading...\")\n",
        "    %cd $p1\n",
        "    !bash /content/drive/MyDrive/Bio_Informatics_RA_Assignment/ena-file-download-read_run-SAMN03431245-fastq_ftp-20250830-1946.sh\n",
        "\n",
        "else:\n",
        "    print(\"Patient1 FASTQs already exist â†’ skipping download.\")\n",
        "\n",
        "# Patient2\n",
        "if not any(os.scandir(p2)):\n",
        "    print(\"Patient2 FASTQs not found â†’ downloading...\")\n",
        "    %cd $p2\n",
        "    !bash /content/drive/MyDrive/Bio_Informatics_RA_Assignment/ena-file-download-read_run-SAMN03431261-fastq_ftp-20250830-2013.sh\n",
        "else:\n",
        "    print(\"Patient2 FASTQs already exist â†’ skipping download.\")\n",
        "\n",
        "# Patient3\n",
        "if not any(os.scandir(p3)):\n",
        "    print(\"Patient3 FASTQs not found â†’ downloading...\")\n",
        "    %cd $p3\n",
        "    !bash /content/drive/MyDrive/Bio_Informatics_RA_Assignment/ena-file-download-selected-files-20250830-2016.sh\n",
        "else:\n",
        "    print(\"Patient3 FASTQs already exist â†’ skipping download.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sia0v7TXQjRr"
      },
      "outputs": [],
      "source": [
        "# For Patient 1 -- I\n",
        "!zcat /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient1/SRR1918236_1.fastq.gz \\\n",
        "/content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient1/SRR1918235_1.fastq.gz \\\n",
        "| gzip > /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient1/Patient1_merged_1.fastq.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoblWsnjTQHq"
      },
      "outputs": [],
      "source": [
        "# For Patient 1 -- II\n",
        "!zcat /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient1/SRR1918236_2.fastq.gz \\\n",
        "/content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient1/SRR1918235_2.fastq.gz \\\n",
        "| gzip > /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient1/Patient1_merged_2.fastq.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjJuT6n1la5F"
      },
      "outputs": [],
      "source": [
        "# For Patient 2 -- I\n",
        "!zcat /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient2/SRR1918241_1.fastq.gz \\\n",
        "/content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient2/SRR1918242_1.fastq.gz \\\n",
        "| gzip > /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient1/Patient2_merged_1.fastq.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y1l0Ut1mH3N"
      },
      "outputs": [],
      "source": [
        "# For Patient 2 -- II\n",
        "!zcat /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient2/SRR1918241_2.fastq.gz \\\n",
        "/content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient2/SRR1918242_2.fastq.gz \\\n",
        "| gzip > /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient1/Patient2_merged_2.fastq.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqBxSjAhmPVN"
      },
      "outputs": [],
      "source": [
        "# For Patient 3 -- I\n",
        "!zcat /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient3/SRR1918235_1.fastq.gz \\\n",
        "/content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient3/SRR1918236_1.fastq.gz \\\n",
        "| gzip > /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient1/Patient3_merged_1.fastq.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBC53ro9mdXF"
      },
      "outputs": [],
      "source": [
        "# For Patient 3 -- II\n",
        "!zcat /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient3/SRR1918235_2.fastq.gz \\\n",
        "/content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient3/SRR1918236_2.fastq.gz \\\n",
        "| gzip > /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient1/Patient3_merged_2.fastq.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "runX6nX-QisX",
        "outputId": "5db1e232-e9cf-4655-94c9-d4fbab4c4e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-02 09:38:36--  https://github.com/liulab-dfci/TRUST4/blob/master/human_IMGT%2BC.fa\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜human_IMGT+C.faâ€™\n",
            "\n",
            "human_IMGT+C.fa         [ <=>                ] 560.04K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-09-02 09:38:36 (4.45 MB/s) - â€˜human_IMGT+C.faâ€™ saved [573486]\n",
            "\n",
            "--2025-09-02 09:38:36--  https://github.com/liulab-dfci/TRUST4/blob/master/hg38_bcrtcr.fa\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜hg38_bcrtcr.faâ€™\n",
            "\n",
            "hg38_bcrtcr.fa          [ <=>                ] 433.21K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-09-02 09:38:37 (3.55 MB/s) - â€˜hg38_bcrtcr.faâ€™ saved [443606]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://github.com/liulab-dfci/TRUST4/blob/master/human_IMGT%2BC.fa\n",
        "!wget -nc https://github.com/liulab-dfci/TRUST4/blob/master/hg38_bcrtcr.fa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S2mNL2bkPJ3"
      },
      "outputs": [],
      "source": [
        "imgt_ref_path = os.path.join(\"/content\", \"human_IMGT+C.fa\")\n",
        "hg38_ref_path = os.path.join(\"/content\", \"hg38_bcrtcr.fa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kn5cAxxQWaG"
      },
      "outputs": [],
      "source": [
        "# Step 1 Pulling the data systematically from the mounted drive\n",
        "base_dir = \"/content/drive/MyDrive/Bio_Informatics_RA_Assignment\"\n",
        "\n",
        "patients = [\"Patient1\", \"Patient2\", \"Patient3\"]\n",
        "\n",
        "patient_files = {\n",
        "    \"Patient1\": (\"Patient1_merged_1.fastq.gz\", \"Patient1_merged_2.fastq.gz\"),\n",
        "    \"Patient2\": (\"Patient2_merged_1.fastq.gz\", \"Patient2_merged_2.fastq.gz\"),\n",
        "    \"Patient3\": (\"Patient3_merged_1.fastq.gz\", \"Patient3_merged_2.fastq.gz\")}\n",
        "\n",
        "# Output dir for results\n",
        "results_dir = os.path.join(base_dir, \"results\")\n",
        "os.makedirs(results_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYrTviq2jVq1"
      },
      "outputs": [],
      "source": [
        "# Optimizing IO\n",
        "def stage_data_locally(patients, patient_files, base_dir):\n",
        "    \"\"\"\n",
        "    Copies necessary FASTQ and reference files from Drive to the local Colab disk.\n",
        "    Returns a dictionary of local FASTQ paths for each patient.\n",
        "    \"\"\"\n",
        "    local_data_dir = \"/content/local_data\"\n",
        "    os.makedirs(local_data_dir, exist_ok=True)\n",
        "\n",
        "    local_paths = {}\n",
        "    for patient in patients:\n",
        "        fastq_dir = os.path.join(base_dir, patient)\n",
        "\n",
        "        # Copy FASTQ files\n",
        "        r1_name, r2_name = patient_files[patient]\n",
        "        local_r1_path = os.path.join(local_data_dir, r1_name)\n",
        "        local_r2_path = os.path.join(local_data_dir, r2_name)\n",
        "\n",
        "        print(f\"Copying {os.path.join(fastq_dir, r1_name)} to {local_r1_path}\")\n",
        "        shutil.copy(os.path.join(fastq_dir, r1_name), local_r1_path)\n",
        "        shutil.copy(os.path.join(fastq_dir, r2_name), local_r2_path)\n",
        "\n",
        "        local_paths[patient] = (local_r1_path, local_r2_path)\n",
        "\n",
        "    print(\"All FASTQ files copied to local disk.\")\n",
        "    return local_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HDfi2i3Oz6w"
      },
      "outputs": [],
      "source": [
        "local_results_dir = \"/content/local_results\"\n",
        "os.makedirs(local_results_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcjQnh6XZ68Y"
      },
      "outputs": [],
      "source": [
        "mixcr_path = \"/usr/local/bin/mixcr\"\n",
        "trust4_path = \"/usr/local/bin/TRUST4/run-trust4\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initial un-chunked parallelized function (good for small files)"
      ],
      "metadata": {
        "id": "iiVvn7iAbm0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5qKa8Imllya"
      },
      "outputs": [],
      "source": [
        "def run_mixcr(patient_name, fastq1, fastq2, output_dir):\n",
        "    \"\"\"\n",
        "    Runs MiXCR analysis for a given patient by explicitly running each step\n",
        "    using the direct executable path and including the required preset.\n",
        "    Ensures parallel processing using all available CPU cores.\n",
        "    \"\"\"\n",
        "    num_threads = os.cpu_count()\n",
        "    if num_threads is None:\n",
        "        num_threads = 1\n",
        "\n",
        "    try:\n",
        "        print(f\"Running MiXCR for {patient_name} with {num_threads} threads...\")\n",
        "        output_prefix = os.path.join(output_dir, f\"{patient_name}_mixcr\")\n",
        "\n",
        "        # Step 1: Align reads\n",
        "        align_cmd = [\n",
        "            mixcr_path, \"align\",\n",
        "            \"--preset\", \"rna-seq\",\n",
        "            \"--species\", \"hsa\",\n",
        "            \"-OreadsLayout=Collinear\",\n",
        "            \"--threads\", str(num_threads),\n",
        "            fastq1, fastq2,\n",
        "            f\"{output_prefix}.vdjca\"\n",
        "        ]\n",
        "        print(f\"Executing: {' '.join(align_cmd)}\")\n",
        "        subprocess.run(align_cmd, check=True)\n",
        "\n",
        "        # Step 2: Assemble clones\n",
        "        assemble_cmd = [\n",
        "            mixcr_path, \"assemble\",\n",
        "            \"--threads\", str(num_threads),\n",
        "            f\"{output_prefix}.vdjca\",\n",
        "            f\"{output_prefix}.clns\"\n",
        "        ]\n",
        "        print(f\"Executing: {' '.join(assemble_cmd)}\")\n",
        "        subprocess.run(assemble_cmd, check=True)\n",
        "\n",
        "        # Step 3: Export clones\n",
        "        export_cmd = [\n",
        "            mixcr_path, \"exportClones\",\n",
        "            f\"{output_prefix}.clns\",\n",
        "            f\"{output_prefix}.clones.txt\"\n",
        "        ]\n",
        "        print(f\"Executing: {' '.join(export_cmd)}\")\n",
        "        subprocess.run(export_cmd, check=True)\n",
        "\n",
        "        print(f\"MiXCR analysis for {patient_name} completed. Output files stored in {output_dir}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running MiXCR for {patient_name}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS0MczT1n2r_"
      },
      "outputs": [],
      "source": [
        "def run_trust4(patient_name, fastq1, fastq2, output_dir):\n",
        "    \"\"\"\n",
        "    Runs TRUST4 analysis for a given patient.\n",
        "    Uses the direct executable path and the correct thread option '-t'.\n",
        "    \"\"\"\n",
        "    num_threads = os.cpu_count()\n",
        "    try:\n",
        "        print(f\"Running TRUST4 for {patient_name} with {num_threads} threads...\")\n",
        "        output_prefix = os.path.join(output_dir, f\"{patient_name}_trust4\")\n",
        "\n",
        "        # Arguments are passed as a list, one element per argument.\n",
        "        cmd = [\n",
        "            \"run-trust4\",\n",
        "            \"-f\", \"/content/hg38_bcrtcr.fa\",\n",
        "            \"--ref\", \"/content/human_IMGT+C.fa\",\n",
        "            \"-1\", fastq1,\n",
        "            \"-2\", fastq2,\n",
        "            \"-o\", output_prefix,\n",
        "            \"--repseq\",\n",
        "            \"-t\", str(num_threads)\n",
        "        ]\n",
        "\n",
        "        # Execute the command\n",
        "        subprocess.run(cmd, check=True)\n",
        "        print(f\"TRUST4 analysis for {patient_name} completed. Output files stored in {output_dir}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running TRUST4 for {patient_name}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UvdUDbNoFhf"
      },
      "outputs": [],
      "source": [
        "def run_catt(patient_name, fastq1, fastq2, output_dir):\n",
        "    \"\"\"\n",
        "    Placeholder function for running CATT..\n",
        "    \"\"\"\n",
        "    print(f\"Running CATT for {patient_name}... (CATT not implemented)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgQyS3h_oMpj"
      },
      "outputs": [],
      "source": [
        "# Outptut Parsers\n",
        "def parse_mixcr_outptut(output_path):\n",
        "  \"\"\"\n",
        "  Parses MiXCR outptut files and returns a list of CDR3 sequences\n",
        "  \"\"\"\n",
        "  clonotypes_df = pd.read_csv(output_path, sep=\"\\t\")\n",
        "  return set(clonotypes_df['CDR3.aa'])\n",
        "\n",
        "def parse_trust4_output(output_path):\n",
        "    \"\"\"\n",
        "    Parses TRUST4 outptut files and returns a list of CDR3 sequences\n",
        "    \"\"\"\n",
        "    clonotypes_df = pd.read_csv(output_path, sep=\"\\t\")\n",
        "    cdr3_sequences = clonotypes_df[clonotypes_df['CDR3aa'].str.len() > 0]['CDR3aa'].tolist()\n",
        "    return set(cdr3_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9s110W8pbQw"
      },
      "outputs": [],
      "source": [
        "# Visualizing Using Venn Dig.\n",
        "def visualize_venn(sets, labels, title):\n",
        "  \"\"\"\n",
        "  Generates and Saves a Three Way Venn Diagram\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(8,8))\n",
        "  venn3(subsets=sets, set_labels=labels)\n",
        "  plt.title(title)\n",
        "  plt.savefig(f\"{title.replace('', '_')}.png\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeLd6d_XqVAA"
      },
      "outputs": [],
      "source": [
        "def get_overlapping_clonotypes(results):\n",
        "  \"\"\"\n",
        "  Analyzes and Visualizes the overlap of clonotypes.\n",
        "  \"\"\"\n",
        "  for patient, tool_results in results.items():\n",
        "    # CDR3 format sets of these two tests\n",
        "    mixcr_set = tool_results.get(\"mixcr\", set())\n",
        "    trust4_set = tool_results.get(\"trust4\", set())\n",
        "    # Keeping CATT just as a placeholder\n",
        "    catt_set = tool_results.get(\"catt\", set())\n",
        "\n",
        "    venn_sets = [mixcr_set, trust4_set, catt_set]\n",
        "    venn_labels = [\"MiXCR\", \"TRUST4\", \"CATT (Not Run YET)\"]\n",
        "    visualize_venn(venn_sets, venn_labels, f\"Clonotype overlap for {patient}\")\n",
        "\n",
        "    all_clonotypes = reduce(set.union, venn_sets)\n",
        "    mixcr_only = mixcr_set - (trust4_set | catt_set)\n",
        "    trust4_only = trust4_set - (mixcr_set | catt_set)\n",
        "    # this is just a placeholder\n",
        "    catt_only = catt_set - (mixcr_set | trust4_set)\n",
        "    mixcr_trust4_overlap = mixcr_set & trust4_set - catt_set\n",
        "\n",
        "    print(f\"Overlap statisitics for {patient} ---\")\n",
        "    print(f\"Total unique clonotypes: {len(all_clonotypes)}\")\n",
        "    print(f\"MiXCR unique: {len(mixcr_only)}\")\n",
        "    print(f\"TRUST4 unique: {len(trust4_only)}\")\n",
        "    print(f\"CATT unique: {len(catt_only)}\")\n",
        "    print(f\"MiXCR & TRUST4 overlap: {len(mixcr_trust4_overlap)}\")\n",
        "\n",
        "    # Non-Overlapping types\n",
        "    print(f\"\\nExample MiXCR-specific clonotypes\")\n",
        "    for clonotype in list(mixcr_only)[:5]:\n",
        "      print(clonotype)\n",
        "\n",
        "    print(f\"\\nExample TRUST4-specific clonotypes\")\n",
        "    for clonotype in list(trust4_only)[:5]:\n",
        "      print(clonotype)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ðŸ”§ MiXCR â€” Optimized Pipeline (Threading Â· Chunking Â· Caching)\n",
        "\n",
        "This section adds a robust, Colab-friendly workflow for **MiXCR**. Key features:\n",
        "\n",
        "- Uses **multi-threading** automatically (`-t` set to all available CPU cores).\n",
        "- **Streams** from `.fastq.gz` directly (no full decompression).\n",
        "- Optional **chunking** for very large FASTQs, with smart per-chunk threading.\n",
        "- **Caching**: skips steps if outputs are already up-to-date.\n",
        "- Clean **logging** + **reports** for reproducibility.\n",
        "\n",
        "> âœ… Recommended free Colab runtime: **CPU** (High-RAM if available). MiXCR does **not** use GPU.\n"
      ],
      "metadata": {
        "id": "1qPvLFCKVugc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vjhCCyDjBZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24bd9f6-b9ed-4d3e-c0c8-63958f9bf7a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$ wget -q https://github.com/shenwei356/seqkit/releases/download/v2.8.2/seqkit_linux_amd64.tar.gz -O /content/seqkit.tar.gz\n",
            "$ tar -xzf /content/seqkit.tar.gz -C /content/\n",
            "$ chmod +x /content/bin/seqkit\n",
            "PATH -> /content/bin:/usr/local/bin/TRUST4:/usr/local/bin:/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n",
            "$ mixcr -v || true\n",
            "$ seqkit version || true\n",
            "\n",
            "âœ… Tooling ready.\n"
          ]
        }
      ],
      "source": [
        "import os, subprocess, sys, shutil, textwrap\n",
        "\n",
        "MIXCR_VERSION = \"4.6.0\"  # change if needed\n",
        "BIN_DIR = \"/content/bin\"\n",
        "os.makedirs(BIN_DIR, exist_ok=True)\n",
        "\n",
        "def run(cmd, **kwargs):\n",
        "    print(\"$\", cmd)\n",
        "    return subprocess.run(cmd, shell=True, check=True, text=True, **kwargs)\n",
        "# SeqKit install (static Linux binary)\n",
        "seqkit_path = os.path.join(BIN_DIR, \"seqkit\")\n",
        "if not os.path.exists(seqkit_path):\n",
        "    run(\"wget -q https://github.com/shenwei356/seqkit/releases/download/v2.8.2/seqkit_linux_amd64.tar.gz -O /content/seqkit.tar.gz\")\n",
        "    run(\"tar -xzf /content/seqkit.tar.gz -C /content/\")\n",
        "    # The tar expands to a directory with the binary inside\n",
        "    found = False\n",
        "    for root, dirs, files in os.walk(\"/content\"):\n",
        "        if \"seqkit\" in files and \"linux_amd64\" in root:\n",
        "            shutil.copy(os.path.join(root, \"seqkit\"), seqkit_path)\n",
        "            found = True\n",
        "            break\n",
        "    if not found:\n",
        "        # fallback: sometimes the tar unpacks directly\n",
        "        if os.path.exists(\"/content/seqkit\"):\n",
        "            shutil.copy(\"/content/seqkit\", seqkit_path)\n",
        "    run(f\"chmod +x {seqkit_path}\")\n",
        "else:\n",
        "    print(\"SeqKit already installed:\", seqkit_path)\n",
        "\n",
        "# Put BIN_DIR on PATH for this session\n",
        "os.environ[\"PATH\"] = f\"{BIN_DIR}:\" + os.environ[\"PATH\"]\n",
        "print(\"PATH ->\", os.environ[\"PATH\"])\n",
        "run(\"mixcr -v || true\")\n",
        "run(\"seqkit version || true\")\n",
        "print(\"\\nâœ… Tooling ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpers: CPU/thread detection, caching utilities, and safe shell runner\n",
        "\n"
      ],
      "metadata": {
        "id": "6v3rsBadWExV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cpu_threads():\n",
        "  try:\n",
        "    n = mp.cpu_count() or 2\n",
        "  except Excetpion as e:\n",
        "    n = 2\n",
        "  return max(1, n)\n",
        "\n",
        "def newer_than(output, *inputs):\n",
        "    out = Path(output)\n",
        "    if not out.exists():\n",
        "        return False\n",
        "    out_mtime = out.stat().st_mtime\n",
        "    for i in inputs:\n",
        "        p = Path(i)\n",
        "        if not p.exists():\n",
        "            return False\n",
        "        if p.stat().st_mtime > out_mtime:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def sh(cmd):\n",
        "    print(f\"$ {cmd}\")\n",
        "    # stream output live\n",
        "    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    for line in proc.stdout:\n",
        "        print(line, end=\"\")\n",
        "    rc = proc.wait()\n",
        "    if rc != 0:\n",
        "        raise RuntimeError(f\"Command failed with exit code {rc}: {cmd}\")\n",
        "    return rc"
      ],
      "metadata": {
        "id": "aByyd9mWWG5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _run(cmd, cwd=None):\n",
        "    \"\"\"Run a shell command with full error surfacing.\"\"\"\n",
        "    print(\"$\", \" \".join(shlex.quote(c) for c in cmd))\n",
        "    res = subprocess.run(cmd, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    if res.returncode != 0:\n",
        "        raise subprocess.CalledProcessError(res.returncode, cmd, output=res.stdout, stderr=res.stderr)\n",
        "    return res\n",
        "\n",
        "def _pair_chunk_files(r1_dir, r2_dir):\n",
        "    r1 = sorted(list(Path(r1_dir).glob(\"*.fastq.gz\")) + list(Path(r1_dir).glob(\"*.fq.gz\")))\n",
        "    r2 = sorted(list(Path(r2_dir).glob(\"*.fastq.gz\")) + list(Path(r2_dir).glob(\"*.fq.gz\")))\n",
        "    if len(r1) != len(r2):\n",
        "        raise ValueError(f\"R1/R2 chunk mismatch: {len(r1)} vs {len(r2)}\")\n",
        "    return list(zip(r1, r2))\n",
        "\n",
        "def _choose_parallelism(n_chunks, total_threads):\n",
        "    \"\"\"\n",
        "    Budget threads safely: threads_per_job * jobs <= total_threads.\n",
        "    Prefer ~2â€“4 threads per chunk if possible.\n",
        "    \"\"\"\n",
        "    total_threads = max(1, int(total_threads or (os.cpu_count() or 1)))\n",
        "    if n_chunks <= 1:\n",
        "        return 1, total_threads  # 1 job, all threads\n",
        "\n",
        "    # start with 4 threads per job if we can afford it\n",
        "    t_per = min(4, total_threads)\n",
        "    jobs = max(1, min(n_chunks, total_threads // t_per))\n",
        "    if jobs == 0:\n",
        "        jobs = 1\n",
        "        t_per = total_threads\n",
        "    # recompute t_per to fully use available threads\n",
        "    t_per = max(1, total_threads // jobs)\n",
        "    return jobs, t_per\n",
        "\n",
        "def _align_one_chunk(mixcr_path, preset, species, reads_layout, r1, r2, out_prefix, threads, report_path):\n",
        "    \"\"\"\n",
        "    Faithful to your working single-run flags.\n",
        "    mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads N R1 R2 out.vdjca\n",
        "    \"\"\"\n",
        "    align_cmd = [\n",
        "        mixcr_path, \"align\",\n",
        "        \"--preset\", preset,\n",
        "        \"--species\", species,\n",
        "        \"-OreadsLayout=\" + reads_layout,\n",
        "        \"--threads\", str(threads),\n",
        "        \"--report\", str(report_path),\n",
        "        str(r1), str(r2),\n",
        "        str(out_prefix) + \".vdjca\",\n",
        "    ]\n",
        "    _run(align_cmd)\n",
        "    return str(out_prefix) + \".vdjca\""
      ],
      "metadata": {
        "id": "YEIUDNhdwyKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MiXCR Core Steps (with caching)"
      ],
      "metadata": {
        "id": "7nqCRLmRW86g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mixcr_align(r1, r2, out_vdjca, report_path=None, threads=None, extra_args=None):\n",
        "    r1, r2, out_vdjca = map(str, (r1, r2, out_vdjca))\n",
        "    threads = threads or cpu_threads()\n",
        "    report = f\"--report {report_path}\" if report_path else \"\"\n",
        "    extra = \" \".join(extra_args) if extra_args else \"\"\n",
        "    if newer_than(out_vdjca, r1, r2):\n",
        "        print(f\"ðŸ” [cache] align up-to-date: {out_vdjca}\")\n",
        "        return out_vdjca\n",
        "    sh(f\"mixcr align -t {threads} {report} {extra} {r1} {r2} {out_vdjca}\")\n",
        "    return out_vdjca\n",
        "\n",
        "def mixcr_assemble(in_vdjca, out_clns, report_path=None, threads=None, extra_args=None):\n",
        "    in_vdjca, out_clns = map(str, (in_vdjca, out_clns))\n",
        "    threads = threads or cpu_threads()\n",
        "    report = f\"--report {report_path}\" if report_path else \"\"\n",
        "    extra = \" \".join(extra_args) if extra_args else \"\"\n",
        "    if newer_than(out_clns, in_vdjca):\n",
        "        print(f\"ðŸ” [cache] assemble up-to-date: {out_clns}\")\n",
        "        return out_clns\n",
        "    sh(f\"mixcr assemble -t {threads} {report} {extra} {in_vdjca} {out_clns}\")\n",
        "    return out_clns\n",
        "\n",
        "def mixcr_export_clones(in_clns, out_txt, preset=\"full\", extra_args=None):\n",
        "    in_clns, out_txt = map(str, (in_clns, out_txt))\n",
        "    extra = \" \".join(extra_args) if extra_args else \"\"\n",
        "    if newer_than(out_txt, in_clns):\n",
        "        print(f\"ðŸ” [cache] export up-to-date: {out_txt}\")\n",
        "        return out_txt\n",
        "    sh(f\"mixcr exportClones --preset {preset} {extra} {in_clns} {out_txt}\")\n",
        "    return out_txt"
      ],
      "metadata": {
        "id": "Y3tnx8VaXDXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking Utilities (SeqKit split2) â€” for very large FASTQs"
      ],
      "metadata": {
        "id": "6IBYAtuyXOeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_fastq_gz(r1_gz, r2_gz, out_dir, chunk_reads=2_000_000, force=False, patient = \"\"):\n",
        "    out_dir = Path(out_dir)\n",
        "    r1_out = out_dir / f\"{patient}chunks_R1\"\n",
        "    r2_out = out_dir / f\"{patient}chunks_R2\"\n",
        "    r1_out.mkdir(parents=True, exist_ok=True)\n",
        "    r2_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # If chunks already exist and force==False, skip splitting\n",
        "    existing = list(r1_out.glob(\"*.fq.gz\")) or list(r1_out.glob(\"*.fastq.gz\"))\n",
        "    if existing and not force:\n",
        "        print(\"ðŸ” [cache] chunks already exist â€” skipping split\")\n",
        "        return r1_out, r2_out\n",
        "\n",
        "    sh(f\"seqkit split2 -s {chunk_reads} -O {r1_out} {r1_gz}\")\n",
        "    sh(f\"seqkit split2 -s {chunk_reads} -O {r2_out} {r2_gz}\")\n",
        "    return r1_out, r2_out\n",
        "\n",
        "def pair_chunk_files(r1_dir, r2_dir):\n",
        "    r1_dir, r2_dir = Path(r1_dir), Path(r2_dir)\n",
        "    r1_files = sorted([p for p in r1_dir.glob('*.fq.gz')] + [p for p in r1_dir.glob('*.fastq.gz')])\n",
        "    r2_files = sorted([p for p in r2_dir.glob('*.fq.gz')] + [p for p in r2_dir.glob('*.fastq.gz')])\n",
        "    # Pair by numeric suffix in filenames (seqkit uses part_001, part_002, ...)\n",
        "    def key(p):\n",
        "        s = p.stem  # remove .gz\n",
        "        s = s.replace(\".fastq\", \"\").replace(\".fq\", \"\")\n",
        "        # extract last number group\n",
        "        import re\n",
        "        m = re.findall(r\"(\\d+)$\", s)\n",
        "        return int(m[-1]) if m else -1\n",
        "    r1_files.sort(key=key)\n",
        "    r2_files.sort(key=key)\n",
        "    if len(r1_files) != len(r2_files):\n",
        "        raise ValueError(\"R1 and R2 chunk counts differ\")\n",
        "    return list(zip(r1_files, r2_files))"
      ],
      "metadata": {
        "id": "Lrd78aM-XRx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Accelerated per-chunk alignment (thread budgeting + caching)"
      ],
      "metadata": {
        "id": "0dkRhg6XZsB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def align_chunks(r1_dir, r2_dir, out_dir, threads_total=None, per_job_min=2, extra_args=None):\n",
        "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    threads_total = threads_total or cpu_threads()\n",
        "    pairs = pair_chunk_files(r1_dir, r2_dir)\n",
        "    n_chunks = len(pairs)\n",
        "    if n_chunks == 0:\n",
        "        raise RuntimeError(\"No chunks found to align\")\n",
        "\n",
        "    # Determine jobs in parallel and threads per job\n",
        "    # Keep per-job threads >= per_job_min, and avoid oversubscription\n",
        "    max_jobs = max(1, threads_total // per_job_min)\n",
        "    jobs = min(max_jobs, n_chunks)\n",
        "    threads_per_job = max(per_job_min, threads_total // jobs)\n",
        "\n",
        "    print(f\"Threads total: {threads_total} | chunks: {n_chunks} | parallel jobs: {jobs} | threads/job: {threads_per_job}\")\n",
        "\n",
        "    futures = []\n",
        "    results = []\n",
        "\n",
        "    def do_align(i, r1, r2):\n",
        "        out_vdjca = out_dir / f\"chunk_{i:04d}.vdjca\"\n",
        "        rep = out_dir / f\"chunk_{i:04d}_align_report.txt\"\n",
        "        mixcr_align(r1, r2, out_vdjca, report_path=rep, threads=threads_per_job, extra_args=extra_args)\n",
        "        return str(out_vdjca)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=jobs) as ex:\n",
        "        for i, (r1, r2) in enumerate(pairs, start=1):\n",
        "            futures.append(ex.submit(do_align, i, r1, r2))\n",
        "        for f in as_completed(futures):\n",
        "            results.append(f.result())\n",
        "\n",
        "    results = sorted(results)\n",
        "    print(f\"âœ… Completed {len(results)} chunk alignments\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "_IQRjxfJZvOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Merge chunked alignments, assemble, and export\n",
        "\n"
      ],
      "metadata": {
        "id": "Tr0Xl45fakOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_and_finalize(vdjca_list, merged_vdjca, clns_path, clones_txt, threads=None, export_preset=\"full\"):\n",
        "    merged_vdjca = str(merged_vdjca); clns_path = str(clns_path); clones_txt = str(clones_txt)\n",
        "    # Cache-aware: if merged_vdjca exists and newer than all parts, skip merge\n",
        "    def parts_newer_than(out, parts):\n",
        "        if not Path(out).exists():\n",
        "            return True\n",
        "        out_m = Path(out).stat().st_mtime\n",
        "        return any(Path(p).stat().st_mtime > out_m for p in parts)\n",
        "\n",
        "    if parts_newer_than(merged_vdjca, vdjca_list):\n",
        "        # mixcr mergeChunks requires vdjca args\n",
        "        args = \" \".join(map(str, vdjca_list))\n",
        "        sh(f\"mixcr mergeChunks {merged_vdjca} {args}\")\n",
        "    else:\n",
        "        print(f\"ðŸ” [cache] merged VDJCA up-to-date: {merged_vdjca}\")\n",
        "\n",
        "    mixcr_assemble(merged_vdjca, clns_path, report_path=str(Path(clns_path).with_suffix('.assemble_report.txt')), threads=threads)\n",
        "    mixcr_export_clones(clns_path, clones_txt, preset=export_preset)\n",
        "    return merged_vdjca, clns_path, clones_txt"
      ],
      "metadata": {
        "id": "zvxCv4kGaleO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Detect read count, and choose chunk size"
      ],
      "metadata": {
        "id": "iqGEBC0ObbO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_reads_fastq_gz(fastq_gz):\n",
        "    # Count lines / 4 = number of reads\n",
        "    cmd = f\"zcat {fastq_gz} | wc -l\"\n",
        "    lines = int(subprocess.check_output(cmd, shell=True).decode().strip())\n",
        "    return lines // 4\n",
        "\n",
        "def choose_chunk_size(total_reads, target_chunks=20, min_chunk=500_000, max_chunk=5_000_000):\n",
        "    chunk = max(min_chunk, min(max_chunk, total_reads // target_chunks))\n",
        "    return int(chunk)"
      ],
      "metadata": {
        "id": "OKhytBCBbaTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#High-level Pipeline: auto mode (single-run or chunked)"
      ],
      "metadata": {
        "id": "IejVJMY3bAC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_mixcr_pipeline(\n",
        "    fastq1, fastq2, work_dir, sample,\n",
        "    chunk_reads=None,                   # e.g., 2_000_000; if None => no chunking\n",
        "    mixcr_path=\"mixcr\",\n",
        "    preset=\"rna-seq\",\n",
        "    species=\"hsa\",\n",
        "    reads_layout=\"Collinear\",\n",
        "    total_threads=None,\n",
        "    force_split=False,\n",
        "    reuse_if_done=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Chunked MiXCR (faithful flags) â†’ assemble once across all chunks â†’ exportClones.\n",
        "    Produces: {work_dir}/{sample}_mixcr.clones.txt\n",
        "\n",
        "    Returns dict with output paths.\n",
        "    \"\"\"\n",
        "    work_dir = Path(work_dir)\n",
        "    work_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    final_clns   = work_dir / f\"{sample}_mixcr.clns\"\n",
        "    final_clones = work_dir / f\"{sample}_mixcr.clones.txt\"\n",
        "    assemble_report = work_dir / f\"{sample}_mixcr_assemble_report.txt\"\n",
        "\n",
        "    # Fast path: reuse final outputs\n",
        "    if reuse_if_done and final_clones.exists() and final_clns.exists():\n",
        "        print(f\"ðŸ” [cache] Found final MiXCR outputs for {sample}, skipping.\")\n",
        "        return {\n",
        "            \"clns\": str(final_clns),\n",
        "            \"clones\": str(final_clones),\n",
        "            \"assemble_report\": str(assemble_report),\n",
        "        }\n",
        "\n",
        "    # --- Prepare chunks (or â€œvirtualâ€ single chunk) ---\n",
        "    if chunk_reads and int(chunk_reads) > 0:\n",
        "        # Safe split with seqkit (must be installed)\n",
        "        chunks_root = work_dir / \"chunks\"\n",
        "        r1_dir = chunks_root / f\"{sample}_chunks_R1\"\n",
        "        r2_dir = chunks_root / f\"{sample}_chunks_R2\"\n",
        "        r1_dir.mkdir(parents=True, exist_ok=True)\n",
        "        r2_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # If already split and not forced, reuse\n",
        "        existing = list(r1_dir.glob(\"*.fastq.gz\")) + list(r1_dir.glob(\"*.fq.gz\"))\n",
        "        if not existing or force_split:\n",
        "            # seqkit split2 -s <reads>\n",
        "            _run([\"seqkit\", \"split2\", \"-s\", str(int(chunk_reads)), \"-O\", str(r1_dir), str(fastq1)])\n",
        "            _run([\"seqkit\", \"split2\", \"-s\", str(int(chunk_reads)), \"-O\", str(r2_dir), str(fastq2)])\n",
        "        pairs = _pair_chunk_files(r1_dir, r2_dir)\n",
        "    else:\n",
        "        # No chunking: treat original pair as a single â€œchunkâ€\n",
        "        pairs = [(Path(fastq1), Path(fastq2))]\n",
        "\n",
        "    n_chunks = len(pairs)\n",
        "    jobs, threads_per_job = _choose_parallelism(n_chunks, total_threads)\n",
        "    print(f\"[{sample}] Chunks: {n_chunks} | parallel jobs: {jobs} | threads/job: {threads_per_job}\")\n",
        "\n",
        "    aligned_dir = work_dir / \"aligned_chunks\"\n",
        "    aligned_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # Align all chunks (parallel)\n",
        "    vdjca_paths = []\n",
        "    errors = []\n",
        "    if n_chunks == 1:\n",
        "        # single run, faithful command\n",
        "        out_prefix  = aligned_dir / f\"chunk_0001\"\n",
        "        report_file = aligned_dir / f\"chunk_0001_align_report.txt\"\n",
        "        try:\n",
        "            vdj = _align_one_chunk(mixcr_path, preset, species, reads_layout,\n",
        "                                   pairs[0][0], pairs[0][1], out_prefix, threads_per_job, report_file)\n",
        "            vdjca_paths.append(vdj)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(e.stderr or e.output)\n",
        "            raise\n",
        "    else:\n",
        "        with ProcessPoolExecutor(max_workers=jobs) as ex:\n",
        "            futs = {}\n",
        "            for i, (r1, r2) in enumerate(pairs, start=1):\n",
        "                out_prefix  = aligned_dir / f\"chunk_{i:04d}\"\n",
        "                report_file = aligned_dir / f\"chunk_{i:04d}_align_report.txt\"\n",
        "                fut = ex.submit(\n",
        "                    _align_one_chunk,\n",
        "                    mixcr_path, preset, species, reads_layout,\n",
        "                    r1, r2, out_prefix, threads_per_job, report_file\n",
        "                )\n",
        "                futs[fut] = str(out_prefix) + \".vdjca\"\n",
        "\n",
        "            for fut in as_completed(futs):\n",
        "                try:\n",
        "                    vdj = fut.result()\n",
        "                    vdjca_paths.append(vdj)\n",
        "                except subprocess.CalledProcessError as e:\n",
        "                    errors.append(e)\n",
        "                    print(e.stderr or e.output)\n",
        "\n",
        "        if errors:\n",
        "            # Surface the first meaningful error\n",
        "            raise RuntimeError(f\"MiXCR align failed for {len(errors)}/{n_chunks} chunks. First error: {errors[0]}\")\n",
        "\n",
        "    # --- Assemble once across ALL chunked alignments ---\n",
        "    # MiXCR assemble accepts multiple .vdjca inputs; last positional is output .clns\n",
        "    assemble_cmd = [mixcr_path, \"assemble\", \"--threads\", str(max(1, int(total_threads or (os.cpu_count() or 1))))] \\\n",
        "                   + vdjca_paths + [str(final_clns)]\n",
        "    # optional report\n",
        "    assemble_cmd = assemble_cmd[:3] + [\"--report\", str(assemble_report)] + assemble_cmd[3:]\n",
        "    _run(assemble_cmd)\n",
        "\n",
        "    # --- Export clones (single output for whole sample) ---\n",
        "    export_cmd = [mixcr_path, \"exportClones\", str(final_clns), str(final_clones)]\n",
        "    _run(export_cmd)\n",
        "\n",
        "    print(f\"[{sample}] MiXCR complete â†’ {final_clones}\")\n",
        "    return {\n",
        "        \"clns\": str(final_clns),\n",
        "        \"clones\": str(final_clones),\n",
        "        \"assemble_report\": str(assemble_report),\n",
        "        \"vdjca_list\": vdjca_paths,\n",
        "    }"
      ],
      "metadata": {
        "id": "wUTE8XEwa-X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_trust4_chunk(fq1_chunk, fq2_chunk, output_prefix, threads=4):\n",
        "    \"\"\"Run TRUST4 on a single chunk of FASTQ data.\"\"\"\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(output_prefix), exist_ok=True)\n",
        "        cmd = [\n",
        "            \"run-trust4\",\n",
        "            \"-f\", \"/content/hg38_bcrtcr.fa\",\n",
        "            \"--ref\", \"/content/human_IMGT+C.fa\",\n",
        "            \"-1\", fq1_chunk,\n",
        "            \"-2\", fq2_chunk,\n",
        "            \"-o\", output_prefix,\n",
        "            \"--repseq\",\n",
        "            \"-t\", str(threads)\n",
        "        ]\n",
        "        subprocess.run(cmd, check=True)\n",
        "        return output_prefix + \".fa\"  # TRUST4 produces TCR contigs as .fa\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running TRUST4 on {fq1_chunk}, {fq2_chunk}: {e}\")\n",
        "        return None\n",
        "\n",
        "def run_trust4_pipeline(chunk_dir1, chunk_dir2, out_dir, prefix, threads=4):\n",
        "    \"\"\"Run TRUST4 in parallel across all chunks and merge results.\"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    fq1_chunks = sorted([os.path.join(chunk_dir1, f) for f in os.listdir(chunk_dir1) if f.endswith(\".fastq.gz\")])\n",
        "    fq2_chunks = sorted([os.path.join(chunk_dir2, f) for f in os.listdir(chunk_dir2) if f.endswith(\".fastq.gz\")])\n",
        "\n",
        "    assert len(fq1_chunks) == len(fq2_chunks), \"Mismatched chunk pairs!\"\n",
        "\n",
        "    tasks = []\n",
        "    for i, (fq1, fq2) in enumerate(zip(fq1_chunks, fq2_chunks)):\n",
        "        chunk_out_prefix = os.path.join(out_dir, f\"{prefix}_chunk{i:04d}\")\n",
        "        tasks.append((fq1, fq2, chunk_out_prefix, threads))\n",
        "\n",
        "    with multiprocessing.Pool(processes=min(threads, len(tasks))) as pool:\n",
        "        results = pool.starmap(run_trust4_chunk, tasks)\n",
        "\n",
        "    # Merge all resulting contigs into one file\n",
        "    merged_file = os.path.join(out_dir, f\"{prefix}_trust4_merged.fa\")\n",
        "    with open(merged_file, \"w\") as fout:\n",
        "        for res in results:\n",
        "            if res and os.path.exists(res):\n",
        "                with open(res, \"r\") as fin:\n",
        "                    fout.write(fin.read())\n",
        "\n",
        "    print(f\"TRUST4 completed. Merged results saved at {merged_file}\")\n",
        "    return merged_file"
      ],
      "metadata": {
        "id": "TnX1oVxv3Oh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQITxMuYoBu9"
      },
      "outputs": [],
      "source": [
        "def process_patient_task(patient, patient_fastqs, base_dir, results_dir):\n",
        "    \"\"\"\n",
        "    A separate function to process a single patient's data, using local FASTQ paths.\n",
        "    \"\"\"\n",
        "    local_fastq_r1, local_fastq_r2 = patient_fastqs\n",
        "    output_patient_dir = os.path.join(results_dir, patient)\n",
        "    os.makedirs(output_patient_dir, exist_ok=True)\n",
        "    print(\"Counting reads this may take a while...\")\n",
        "    reads = count_reads_fastq_gz(local_fastq_r1)\n",
        "    print(f\"Patient {patient} has {reads} reads\")\n",
        "    chunk_size = choose_chunk_size(reads)\n",
        "    print(f\"Choosing chunk size: {chunk_size}\")\n",
        "    # MiXCR TEST\n",
        "    try:\n",
        "        print(\"Running MiXCR Test...\")\n",
        "        # run_mixcr(patient, local_fastq_r1, local_fastq_r2, output_patient_dir)\n",
        "        run_mixcr_pipeline(local_fastq_r1, local_fastq_r2, output_patient_dir, sample=patient, chunk_reads=chunk_size)\n",
        "    except Exception as e1:\n",
        "        print(f\"Error During MiXCR Test for {patient}: {e1}\")\n",
        "\n",
        "    # TRUST4 TEST\n",
        "    try:\n",
        "        # run_trust4(patient, local_fastq_r1, local_fastq_r2, output_patient_dir)\n",
        "        run_trust4_pipeline(local_fastq_r1, local_fastq_r2, output_patient_dir, prefix=patient, threads=os.cpu_count())\n",
        "    except Exception as e2:\n",
        "        print(f\"Error During TRUST4 Test for {patient}: {e2}\")\n",
        "\n",
        "    mixcr_output_path = os.path.join(output_patient_dir, f\"{patient}_mixcr.clones.txt\")\n",
        "    trust4_output_path = os.path.join(output_patient_dir, f\"{patient}_trust4_report.tsv\")\n",
        "\n",
        "    patient_results = {}\n",
        "    if os.path.exists(mixcr_output_path):\n",
        "        try:\n",
        "            patient_results[\"mixcr\"] = parse_mixcr_output(mixcr_output_path)\n",
        "            print(f\"Parsed MiXCR output: {len(patient_results['mixcr'])} clonotypes\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing MiXCR output for {patient}: {e}\")\n",
        "\n",
        "    if os.path.exists(trust4_output_path):\n",
        "        try:\n",
        "            patient_results[\"trust4\"] = parse_trust4_output(trust4_output_path)\n",
        "            print(f\"Parsed trust4 output: {len(patient_results['trust4'])} clonotypes\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing TRUST4 output for {patient}: {e}\")\n",
        "\n",
        "    return {patient: patient_results}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX9O3mcuti2o"
      },
      "outputs": [],
      "source": [
        "def run_tcr_pipeline(patients, patient_files, base_dir, results_dir):\n",
        "    \"\"\"\n",
        "    TCR Analysis Pipeline with parallel processing and local data staging.\n",
        "    \"\"\"\n",
        "    # Stage data locally before starting parallel processes\n",
        "    local_fastq_paths = stage_data_locally(patients, patient_files, base_dir)\n",
        "\n",
        "    all_results = {}\n",
        "    with ProcessPoolExecutor(max_workers=len(patients)) as executor:\n",
        "        from functools import partial\n",
        "        func = partial(process_patient_task, base_dir=base_dir, results_dir=results_dir)\n",
        "\n",
        "        tasks = [(patient, local_fastq_paths[patient]) for patient in patients]\n",
        "\n",
        "        for result in executor.map(func, [task[0] for task in tasks], [task[1] for task in tasks]):\n",
        "            all_results.update(result)\n",
        "\n",
        "    get_overlapping_clonotypes(all_results)\n",
        "    print(\"\\nAnalysis Complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1LFAwyzMmNd",
        "outputId": "0f7ab406-37a0-408f-d317-900ca5ec7258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling main pipeline function\n",
            "Copying /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient1/Patient1_merged_1.fastq.gz to /content/local_data/Patient1_merged_1.fastq.gz\n",
            "Copying /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient2/Patient2_merged_1.fastq.gz to /content/local_data/Patient2_merged_1.fastq.gz\n",
            "Copying /content/drive/MyDrive/Bio_Informatics_RA_Assignment/Patient3/Patient3_merged_1.fastq.gz to /content/local_data/Patient3_merged_1.fastq.gz\n",
            "All FASTQ files copied to local disk.\n",
            "Counting reads this may take a while...\n",
            "Counting reads this may take a while...Counting reads this may take a while...\n",
            "\n",
            "Patient Patient2 has 87508867 reads\n",
            "Choosing chunk size: 4375443\n",
            "Running MiXCR Test...\n",
            "$ seqkit split2 -s 4375443 -O /content/local_results/Patient2/chunks/Patient2_chunks_R1 /content/local_data/Patient2_merged_1.fastq.gz\n",
            "Patient Patient3 has 101745993 reads\n",
            "Choosing chunk size: 5000000\n",
            "Running MiXCR Test...\n",
            "$ seqkit split2 -s 5000000 -O /content/local_results/Patient3/chunks/Patient3_chunks_R1 /content/local_data/Patient3_merged_1.fastq.gz\n",
            "Patient Patient1 has 101745993 reads\n",
            "Choosing chunk size: 5000000\n",
            "Running MiXCR Test...\n",
            "$ seqkit split2 -s 5000000 -O /content/local_results/Patient1/chunks/Patient1_chunks_R1 /content/local_data/Patient1_merged_1.fastq.gz\n",
            "$ seqkit split2 -s 4375443 -O /content/local_results/Patient2/chunks/Patient2_chunks_R2 /content/local_data/Patient2_merged_2.fastq.gz\n",
            "$ seqkit split2 -s 5000000 -O /content/local_results/Patient3/chunks/Patient3_chunks_R2 /content/local_data/Patient3_merged_2.fastq.gz\n",
            "$ seqkit split2 -s 5000000 -O /content/local_results/Patient1/chunks/Patient1_chunks_R2 /content/local_data/Patient1_merged_2.fastq.gz\n",
            "[Patient2] Chunks: 21 | parallel jobs: 21 | threads/job: 4\n",
            "$$$$$$$$   $$$   mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0001_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_001.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_001.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0001.vdjca $$$mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0003_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_003.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_003.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0003.vdjca$$$$mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0002_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_002.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_002.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0002.vdjca $$$  mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0004_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_004.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_004.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0004.vdjcamixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0005_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_005.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_005.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0005.vdjca \n",
            "mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0009_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_009.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_009.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0009.vdjca mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0011_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_011.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_011.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0011.vdjca\n",
            "    \n",
            "  mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0007_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_007.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_007.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0007.vdjca mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0013_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_013.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_013.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0013.vdjcamixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0012_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_012.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_012.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0012.vdjca  \n",
            "\n",
            "mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0008_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_008.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_008.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0008.vdjca\n",
            "mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0019_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_019.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_019.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0019.vdjca\n",
            "mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0006_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_006.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_006.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0006.vdjcamixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0014_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_014.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_014.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0014.vdjcamixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0020_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_020.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_020.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0020.vdjcamixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0010_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_010.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_010.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0010.vdjcamixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0015_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_015.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_015.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0015.vdjcamixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0021_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_021.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_021.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0021.vdjca\n",
            "\n",
            "mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0016_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_016.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_016.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0016.vdjca\n",
            "mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0017_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_017.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_017.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0017.vdjca\n",
            "mixcr align --preset rna-seq --species hsa -OreadsLayout=Collinear --threads 4 --report /content/local_results/Patient2/aligned_chunks/chunk_0018_align_report.txt /content/local_results/Patient2/chunks/Patient2_chunks_R1/Patient2_merged_1.part_018.fastq.gz /content/local_results/Patient2/chunks/Patient2_chunks_R2/Patient2_merged_2.part_018.fastq.gz /content/local_results/Patient2/aligned_chunks/chunk_0018.vdjca\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Download reference files first if not already done\n",
        "    if not os.path.exists(imgt_ref_path):\n",
        "        print(\"Downloading reference files...\")\n",
        "        !wget -nc -P \"/content\" https://github.com/liulab-dfci/TRUST4/raw/master/vdjc_db/human_IMGT+C.fa\n",
        "    if not os.path.exists(hg38_ref_path):\n",
        "        print(\"Downloading reference files...\")\n",
        "        !wget -nc -P \"/content\" https://github.com/liulab-dfci/TRUST4/raw/master/ref_genome/hg38_bcrtcr.fa\n",
        "\n",
        "    print(\"Calling main pipeline function\")\n",
        "    run_tcr_pipeline(patients, patient_files, base_dir, local_results_dir)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2972b74c"
      },
      "source": [
        "# Create a requirements.txt file\n",
        "!pip freeze > requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}