{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377d62b-d1f2-41c8-a082-bdfda7c506ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - bioconda\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.3.1\n",
      "    latest version: 25.7.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/atharva/miniforge/envs/rna_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - matplotlib-venn\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    brotli-1.1.0               |       hb03c661_4          19 KB  conda-forge\n",
      "    brotli-bin-1.1.0           |       hb03c661_4          19 KB  conda-forge\n",
      "    contourpy-1.3.3            |  py313h7037e92_2         290 KB  conda-forge\n",
      "    cycler-0.12.1              |     pyhd8ed1ab_1          13 KB  conda-forge\n",
      "    fonttools-4.59.2           |  py313h3dea7bd_0         2.8 MB  conda-forge\n",
      "    kiwisolver-1.4.9           |  py313hc8edb43_1          75 KB  conda-forge\n",
      "    libblas-3.9.0              |35_h4a7cf45_openblas          17 KB  conda-forge\n",
      "    libbrotlicommon-1.1.0      |       hb03c661_4          68 KB  conda-forge\n",
      "    libbrotlidec-1.1.0         |       hb03c661_4          33 KB  conda-forge\n",
      "    libbrotlienc-1.1.0         |       hb03c661_4         283 KB  conda-forge\n",
      "    libcblas-3.9.0             |35_h0358290_openblas          17 KB  conda-forge\n",
      "    libgfortran-15.1.0         |       h69a702a_5          28 KB  conda-forge\n",
      "    libgfortran5-15.1.0        |       hcea5267_5         1.5 MB  conda-forge\n",
      "    liblapack-3.9.0            |35_h47877c9_openblas          17 KB  conda-forge\n",
      "    libopenblas-0.3.30         |pthreads_h94d23a6_2         5.7 MB  conda-forge\n",
      "    matplotlib-base-3.10.6     |  py313h683a580_1         8.1 MB  conda-forge\n",
      "    matplotlib-venn-1.1.2      |     pyhd8ed1ab_0          39 KB  conda-forge\n",
      "    munkres-1.1.4              |     pyhd8ed1ab_1          15 KB  conda-forge\n",
      "    numpy-2.3.3                |  py313hf6604e3_0         8.5 MB  conda-forge\n",
      "    openjpeg-2.5.3             |       h55fea9a_1         349 KB  conda-forge\n",
      "    pillow-11.3.0              |  py313hf46931b_1        40.6 MB  conda-forge\n",
      "    pyparsing-3.2.4            |     pyhcf101f3_0         102 KB  conda-forge\n",
      "    qhull-2020.2               |       h434a139_5         540 KB  conda-forge\n",
      "    scipy-1.16.2               |  py313h11c21cd_0        16.2 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        85.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  brotli             conda-forge/linux-64::brotli-1.1.0-hb03c661_4 \n",
      "  brotli-bin         conda-forge/linux-64::brotli-bin-1.1.0-hb03c661_4 \n",
      "  contourpy          conda-forge/linux-64::contourpy-1.3.3-py313h7037e92_2 \n",
      "  cycler             conda-forge/noarch::cycler-0.12.1-pyhd8ed1ab_1 \n",
      "  fonttools          conda-forge/linux-64::fonttools-4.59.2-py313h3dea7bd_0 \n",
      "  kiwisolver         conda-forge/linux-64::kiwisolver-1.4.9-py313hc8edb43_1 \n",
      "  libblas            conda-forge/linux-64::libblas-3.9.0-35_h4a7cf45_openblas \n",
      "  libbrotlicommon    conda-forge/linux-64::libbrotlicommon-1.1.0-hb03c661_4 \n",
      "  libbrotlidec       conda-forge/linux-64::libbrotlidec-1.1.0-hb03c661_4 \n",
      "  libbrotlienc       conda-forge/linux-64::libbrotlienc-1.1.0-hb03c661_4 \n",
      "  libcblas           conda-forge/linux-64::libcblas-3.9.0-35_h0358290_openblas \n",
      "  libgfortran        conda-forge/linux-64::libgfortran-15.1.0-h69a702a_5 \n",
      "  libgfortran5       conda-forge/linux-64::libgfortran5-15.1.0-hcea5267_5 \n",
      "  liblapack          conda-forge/linux-64::liblapack-3.9.0-35_h47877c9_openblas \n",
      "  libopenblas        conda-forge/linux-64::libopenblas-0.3.30-pthreads_h94d23a6_2 \n",
      "  matplotlib-base    conda-forge/linux-64::matplotlib-base-3.10.6-py313h683a580_1 \n",
      "  matplotlib-venn    conda-forge/noarch::matplotlib-venn-1.1.2-pyhd8ed1ab_0 \n",
      "  munkres            conda-forge/noarch::munkres-1.1.4-pyhd8ed1ab_1 \n",
      "  numpy              conda-forge/linux-64::numpy-2.3.3-py313hf6604e3_0 \n",
      "  openjpeg           conda-forge/linux-64::openjpeg-2.5.3-h55fea9a_1 \n",
      "  pillow             conda-forge/linux-64::pillow-11.3.0-py313hf46931b_1 \n",
      "  pyparsing          conda-forge/noarch::pyparsing-3.2.4-pyhcf101f3_0 \n",
      "  qhull              conda-forge/linux-64::qhull-2020.2-h434a139_5 \n",
      "  scipy              conda-forge/linux-64::scipy-1.16.2-py313h11c21cd_0 \n",
      "\n",
      "\n",
      "Proceed ([y]/n)? "
     ]
    }
   ],
   "source": [
    "!conda install matplotlib-venn\n",
    "!conda install pandas numpy matplotlib seaborn scikit-learn matplotlib-venn pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bfee56a-2a77-411c-a348-e19a71a5a7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.11b\n"
     ]
    }
   ],
   "source": [
    "!STAR --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ef15f4-3d03-4c60-85d7-17d35f97c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from matplotlib_venn import venn3\n",
    "from functools import reduce\n",
    "import shutil\n",
    "import sys\n",
    "import os, subprocess, shlex\n",
    "# Parallel Processing with Multiprocessing\n",
    "import gzip\n",
    "from multiprocessing import Pool, cpu_count\n",
    "# from Bio import SeqIO\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "import glob, math\n",
    "import shlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a475b1de-f9c2-42fe-b5c7-5ee09f0f9c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient1 FASTQs already exist → skipping download.\n"
     ]
    }
   ],
   "source": [
    "p1 = \"/mnt/a/Projects/RA_Assignment/Patient1\"\n",
    "\n",
    "# Patient1\n",
    "if not any(os.scandir(p1)):  # empty dir\n",
    "    print(\"Patient1 FASTQs not found → downloading...\")\n",
    "    %cd $p1\n",
    "    !bash /content/drive/MyDrive/Bio_Informatics_RA_Assignment/ena-file-download-read_run-SAMN03431245-fastq_ftp-20250830-1946.sh\n",
    "\n",
    "else:\n",
    "    print(\"Patient1 FASTQs already exist → skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c371a0ce-b707-4585-b70c-979fb7673e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fastq_head(file_path, num_reads=5):\n",
    "  if( file_path.endswith('.gz')):\n",
    "    open_func = gzip.open\n",
    "    mode = 'rt'\n",
    "  else:\n",
    "    open_func = open\n",
    "    mode = 'r'\n",
    "  with open_func(file_path, mode) as f:\n",
    "    for i in range(num_reads * 4):\n",
    "      line = f.readline()\n",
    "      if not line:\n",
    "        break\n",
    "      print(line.rstrip())\n",
    "\n",
    "import gzip\n",
    "\n",
    "def get_fastq_read_count(file_path):\n",
    "    \"\"\"\n",
    "    Counts the total number of reads in a FASTQ file.\n",
    "    Handles both gzipped (.gz) and uncompressed files.\n",
    "    \"\"\"\n",
    "    if file_path.endswith('.gz'):\n",
    "        open_func = gzip.open\n",
    "        mode = 'rt'\n",
    "    else:\n",
    "        open_func = open\n",
    "        mode = 'r'\n",
    "\n",
    "    total_reads = 0\n",
    "    with open_func(file_path, mode) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if (i + 1) % 4 == 0:\n",
    "                total_reads += 1\n",
    "    \n",
    "    return total_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0dd242b-6915-481e-abbd-0b3eb5969a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@SRR1918235.1 HWI-ST766:119:C4N1AACXX:3:1114:4077:54247/1\n",
      "CCGAAATCTGTGCAGAGGAGAACGCAGCTCCGCCCTCGCGGTGCTCTCCGGGTCTGTGCTGAGGAGAACGCAACTCCGCCGAGATCGGAAGAGCACACGT\n",
      "+\n",
      "BBBFBFFFBBBFBBFBBF<FFFBBBBFFBBBF<<FFFFB'0<BBB0BFFB007<BB<B<<<<B7BB<B<BBBB7B0'0'07<B<BBBB0B7707BB0B##\n",
      "@SRR1918235.2 HWI-ST766:119:C4N1AACXX:3:2214:9166:51017/1\n",
      "CCCACCAGCAATGTCTAGGAATGCCTGTTTCTCCACAAAGTGTTTACTTTTGGATTTTTGCCAGTCTAACAGGTGAAGCCCTGGAAATTCTTATTAGTGA\n",
      "+\n",
      "BBBFFFFFFFFBFFFFFIIFFFFFIFFFFFFFBFFFBFIFBFFFFFIIFFIFFIIIIFFIBFIIFFFIBFIBFBFBB7B<BFBBFBBFBBFFFFFFFBB<\n"
     ]
    }
   ],
   "source": [
    "print_fastq_head('/mnt/a/Projects/RA_Assignment/Patient1/SRR1918235_1.fastq.gz',2)\n",
    "# total_reads = get_fastq_read_count('A:\\\\Projects\\\\RA_Assignment\\\\Patient1\\\\SRR1918235_1.fastq.gz')\n",
    "# print(f\"Total Number of reads: {total_reads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9426fd5-3c7b-4734-ba6b-6ed3152659dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /home/atharva/miniforge/envs/rna_env/bin/python\n",
      "Kernel PATH: /home/atharva/miniforge/envs/rna_env/bin:/home/atharva/miniforge/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files/Eclipse Adoptium/jdk-17.0.16.8-hotspot/bin:/mnt/c/Program Files (x86)/Common Files/Intel/Shared Libraries/redist/intel64/compiler:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA App/NvDLISR:/mnt/c/Program Files/Git/cmd:/mnt/c/Users/athar/AppData/Local/Programs/Python/Python311/Lib/site-packages/pip:/mnt/c/Users/athar/AppData/Local/Programs/Python/Python311/Scripts:/mnt/c/Users/athar/AppData/Roaming/nvm:/mnt/c/Program Files/nodejs:/mnt/c/Program Files/Microsoft SQL Server/150/Tools/Binn:/mnt/c/Program Files/Microsoft SQL Server/Client SDK/ODBC/170/Tools/Binn:/mnt/c/Program Files/dotnet:/mnt/c/Program Files (x86)/Microsoft SQL Server/160/Tools/Binn:/mnt/c/Program Files/Microsoft SQL Server/160/Tools/Binn:/mnt/c/Program Files/Microsoft SQL Server/160/DTS/Binn:/mnt/c/Program Files (x86)/Microsoft SQL Server/160/DTS/Binn:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/OpenSSH:/mnt/c/Program Files/nodejs:/mnt/c/Users/athar/AppData/Local/Programs/Python/Python311/Lib/site-packages/pandoc:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/PuTTY:/mnt/c/Program Files/Docker/Docker/resources/bin:/mnt/c/Users/athar/OneDrive/Documents/mixcr-4.6.0/mixcr.jar:/mnt/c/Windows/System32/seqkit:/mnt/c/Program Files/Terraform:/mnt/c/Users/athar/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/athar/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Program Files/JetBrains/IntelliJ IDEA 2023.2.3/bin:/mnt/c/Program Files/JetBrains/IntelliJ IDEA Community Edition 2023.2.3/bin:/mnt/c/Program Files/curl-8.6.0_4-win64-mingw/bin:/mnt/c/Users/athar/AppData/Local/GitHubDesktop/bin:/mnt/c/Users/athar/AppData/Local/Programs/Python/Python311/Lib/site-packages/pip (python 3.11):/mnt/c/Program Files/JetBrains/PyCharm Community Edition 2024.1.2/bin:/mnt/c/Users/athar/.dotnet/tools:/mnt/c/Users/athar/.cache/lm-studio/bin:/mnt/c/Users/athar/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/athar/AppData/Roaming/npm:/mnt/c/Users/athar/AppData/Local/Programs/Ollama:/mnt/c/Users/athar/AppData/Local/Pandoc:/mnt/c/Users/athar/.dotnet/tools:/mnt/c/ProgramData/mingw64/mingw64/bin:/snap/bin:/home/atharva/A:/Projects/RA_Assignment/STAR-2.7.11b/source/STAR\n",
      "shutil.which('fastqc') -> /home/atharva/miniforge/envs/rna_env/bin/fastqc\n",
      "shutil.which('STAR') -> /home/atharva/miniforge/envs/rna_env/bin/STAR\n",
      "shutil.which('samtools') -> /home/atharva/miniforge/envs/rna_env/bin/samtools\n",
      "shutil.which('featureCounts') -> /home/atharva/miniforge/envs/rna_env/bin/featureCounts\n",
      "fastqc --version: FastQC v0.12.1\n"
     ]
    }
   ],
   "source": [
    "import shutil, sys, os, subprocess\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Kernel PATH:\", os.environ.get(\"PATH\"))\n",
    "print(\"shutil.which('fastqc') ->\", shutil.which(\"fastqc\"))\n",
    "print(\"shutil.which('STAR') ->\", shutil.which(\"STAR\"))\n",
    "print(\"shutil.which('samtools') ->\", shutil.which(\"samtools\"))\n",
    "print(\"shutil.which('featureCounts') ->\", shutil.which(\"featureCounts\"))\n",
    "\n",
    "# Try running fastqc --version (capture output)\n",
    "try:\n",
    "    out = subprocess.run([\"fastqc\",\"--version\"], capture_output=True, text=True, check=True)\n",
    "    print(\"fastqc --version:\", out.stdout.strip() or out.stderr.strip())\n",
    "except Exception as e:\n",
    "    print(\"Error running fastqc --version:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2fc3286-8473-434c-b3f6-b73da1c8dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shlex\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "class RNASeqPipeline:\n",
    "    \"\"\"RNA-seq analysis pipeline for gene expression data (not TCR/BCR)\"\"\"\n",
    "    \n",
    "    def __init__(self, star_path=\"STAR\", samtools_path=\"samtools\", \n",
    "                 featurecounts_path=\"featureCounts\", total_threads=None):\n",
    "        self.star_path = star_path\n",
    "        self.samtools_path = samtools_path\n",
    "        self.featurecounts_path = featurecounts_path\n",
    "        self.total_threads = total_threads or os.cpu_count() or 4\n",
    "        \n",
    "    def run_command(self, cmd, cwd=None, timeout=None, env=None):\n",
    "        \"\"\"\n",
    "        Run a command safely.\n",
    "        - cmd: list (recommended) or string\n",
    "        - returns subprocess.CompletedProcess on success\n",
    "        - raises CalledProcessError on failure\n",
    "        \"\"\"\n",
    "        import subprocess, shlex, time\n",
    "        print(\"\\n$ \" + (shlex.join(cmd) if isinstance(cmd, (list,tuple)) else str(cmd)))\n",
    "        try:\n",
    "            if isinstance(cmd, (list, tuple)):\n",
    "                # run directly without shell (safer)\n",
    "                result = subprocess.run(cmd, cwd=cwd, check=True, capture_output=True, text=True, timeout=timeout, env=env)\n",
    "            else:\n",
    "                # cmd is a string; run via shell\n",
    "                result = subprocess.run(cmd, cwd=cwd, check=True, capture_output=True, text=True, timeout=timeout, shell=True, env=env)\n",
    "            if result.stdout:\n",
    "                print(result.stdout)\n",
    "            if result.stderr:\n",
    "                # print warnings but not fail\n",
    "                print(result.stderr)\n",
    "            return result\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Command failed (returncode={e.returncode})\")\n",
    "            if e.stdout:\n",
    "                print(\"STDOUT:\", e.stdout)\n",
    "            if e.stderr:\n",
    "                print(\"STDERR:\", e.stderr)\n",
    "            raise\n",
    "        except subprocess.TimeoutExpired as e:\n",
    "            print(f\"Command timed out after {timeout} seconds\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    def check_tools_installed(self):\n",
    "        \"\"\"Check if required CLI tools are available in PATH (kernel env).\"\"\"\n",
    "        import shutil, subprocess\n",
    "        tools = [\n",
    "            (\"STAR\", \"STAR aligner\"),\n",
    "            (\"samtools\", \"SAMtools\"),\n",
    "            (\"featureCounts\", \"featureCounts (subread)\"),\n",
    "            (\"fastqc\", \"FastQC\"),\n",
    "            (\"seqkit\", \"SeqKit\")\n",
    "        ]\n",
    "        missing = []\n",
    "        for exe, nice in tools:\n",
    "            path = shutil.which(exe)\n",
    "            if path:\n",
    "                # try quick version check when possible\n",
    "                try:\n",
    "                    subprocess.run([exe, \"--version\"], capture_output=True, check=False, timeout=10)\n",
    "                    print(f\"✓ {nice} found: {path}\")\n",
    "                except Exception:\n",
    "                    print(f\"✓ {nice} found: {path} (version probe failed but binary exists)\")\n",
    "            else:\n",
    "                print(f\"✗ {nice} ({exe}) not found in PATH\")\n",
    "                missing.append(f\"{nice} ({exe})\")\n",
    "        if missing:\n",
    "            print(\"\\nMissing tools:\", \", \".join(missing))\n",
    "            print(\"If they are installed in a conda env, start Jupyter from that env or add their bin directory to PATH.\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    \n",
    "    def quality_control(self, fastq1, fastq2, output_dir, threads=None):\n",
    "        \"\"\"Run FastQC in non-interactive CLI mode and produce HTML outputs in output_dir.\"\"\"\n",
    "        from pathlib import Path\n",
    "        import shutil\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        threads = threads or min(4, self.total_threads)\n",
    "        fastqc_exe = shutil.which(\"fastqc\")\n",
    "        if not fastqc_exe:\n",
    "            raise FileNotFoundError(\"FastQC not found in PATH (check environment).\")\n",
    "        print(\"Running FastQC (non-interactive)...\")\n",
    "        cmd = [fastqc_exe, \"-o\", str(output_dir), \"-t\", str(threads), str(fastq1), str(fastq2)]\n",
    "        self.run_command(cmd, timeout=1800)\n",
    "        # locate generated HTML (fastqc names output like <sample>_fastqc.html)\n",
    "        # Optionally return the paths so the caller can embed them\n",
    "        htmls = list(output_dir.glob(\"*_fastqc.html\"))\n",
    "        zips = list(output_dir.glob(\"*_fastqc.zip\"))\n",
    "        print(f\"FastQC produced {len(htmls)} HTML report(s) and {len(zips)} zip(s)\")\n",
    "        return {\"htmls\": [str(p) for p in htmls], \"zips\": [str(p) for p in zips]}\n",
    "\n",
    "    \n",
    "    def basic_read_stats(self, fastq1, fastq2):\n",
    "        \"\"\"Get basic statistics about the reads\"\"\"\n",
    "        print(\"Analyzing read statistics...\")\n",
    "        \n",
    "        # Count reads\n",
    "        result1 = subprocess.run(\n",
    "            [\"seqkit\", \"stats\", str(fastq1)], \n",
    "            capture_output=True, text=True, shell=True\n",
    "        )\n",
    "        result2 = subprocess.run(\n",
    "            [\"seqkit\", \"stats\", str(fastq2)], \n",
    "            capture_output=True, text=True, shell=True\n",
    "        )\n",
    "        \n",
    "        print(\"Read 1 stats:\")\n",
    "        print(result1.stdout)\n",
    "        print(\"Read 2 stats:\")\n",
    "        print(result2.stdout)\n",
    "        \n",
    "        return result1.stdout, result2.stdout\n",
    "    \n",
    "    def download_reference_genome(self, output_dir, species=\"human\", retries=3, chunk_size=1024*1024):\n",
    "        \"\"\"\n",
    "        Download reference genome and GTF using Python (no wget).\n",
    "        Streams to disk and avoids shell/wget issues.\n",
    "    \n",
    "        Returns: (genome_fa_path, gtf_unzipped_path)\n",
    "        \"\"\"\n",
    "        from pathlib import Path\n",
    "        import urllib.request\n",
    "        import shutil\n",
    "        import gzip\n",
    "        import time\n",
    "        import sys\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "        if species.lower() != \"human\":\n",
    "            raise ValueError(\"Only human reference genome is implemented\")\n",
    "    \n",
    "        # URLs (Gencode release 44 as in original)\n",
    "        genome_url = \"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_44/GRCh38.primary_assembly.genome.fa.gz\"\n",
    "        gtf_url = \"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_44/gencode.v44.primary_assembly.annotation.gtf.gz\"\n",
    "    \n",
    "        genome_gz = output_dir / \"GRCh38.primary_assembly.genome.fa.gz\"\n",
    "        gtf_gz = output_dir / \"gencode.v44.primary_assembly.annotation.gtf.gz\"\n",
    "    \n",
    "        def _download(url, target, retries=retries):\n",
    "            attempt = 0\n",
    "            while attempt < retries:\n",
    "                try:\n",
    "                    attempt += 1\n",
    "                    print(f\"Downloading (attempt {attempt}): {url}\")\n",
    "                    req = urllib.request.urlopen(url, timeout=60)\n",
    "                    total = req.getheader('Content-Length')\n",
    "                    if total:\n",
    "                        total = int(total)\n",
    "                        print(f\"Total bytes: {total:,}\")\n",
    "                    with open(target, \"wb\") as out:\n",
    "                        downloaded = 0\n",
    "                        while True:\n",
    "                            chunk = req.read(chunk_size)\n",
    "                            if not chunk:\n",
    "                                break\n",
    "                            out.write(chunk)\n",
    "                            downloaded += len(chunk)\n",
    "                            if total:\n",
    "                                pct = downloaded/total*100\n",
    "                                sys.stdout.write(f\"\\r{downloaded:,}/{total:,} bytes ({pct:.1f}%)\")\n",
    "                            else:\n",
    "                                sys.stdout.write(f\"\\r{downloaded:,} bytes\")\n",
    "                            sys.stdout.flush()\n",
    "                    print(\"\\nDownload finished:\", target)\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nDownload attempt {attempt} failed: {e}\")\n",
    "                    time.sleep(3 * attempt)\n",
    "            raise RuntimeError(f\"Failed to download {url} after {retries} attempts\")\n",
    "    \n",
    "        # download files only if not already present\n",
    "        if not genome_gz.exists():\n",
    "            _download(genome_url, genome_gz)\n",
    "        else:\n",
    "            print(\"Genome gz already exists, skipping download:\", genome_gz)\n",
    "    \n",
    "        if not gtf_gz.exists():\n",
    "            _download(gtf_url, gtf_gz)\n",
    "        else:\n",
    "            print(\"GTF gz already exists, skipping download:\", gtf_gz)\n",
    "    \n",
    "        # Decompress gz -> .fa and .gtf (if not already exist)\n",
    "        genome_fa = output_dir / \"GRCh38.primary_assembly.genome.fa\"\n",
    "        gtf_unzipped = output_dir / \"gencode.v44.primary_assembly.annotation.gtf\"\n",
    "    \n",
    "        if not genome_fa.exists():\n",
    "            print(\"Decompressing genome fasta (this will keep .gz file).\")\n",
    "            with gzip.open(genome_gz, \"rb\") as f_in, open(genome_fa, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        else:\n",
    "            print(\"Genome fasta already exists:\", genome_fa)\n",
    "    \n",
    "        if not gtf_unzipped.exists():\n",
    "            print(\"Decompressing GTF.\")\n",
    "            with gzip.open(gtf_gz, \"rb\") as f_in, open(gtf_unzipped, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        else:\n",
    "            print(\"GTF already exists:\", gtf_unzipped)\n",
    "    \n",
    "        return str(genome_fa), str(gtf_unzipped)\n",
    "\n",
    "    \n",
    "    def build_star_index(self, genome_fasta, gtf_file, index_dir, max_threads=None):\n",
    "        \"\"\"\n",
    "        Robust STAR index builder with fallbacks for memory-limited environments.\n",
    "        Tries:\n",
    "          1) normal build with capped threads\n",
    "          2) retry with --limitGenomeGenerateRAM and fewer threads\n",
    "          3) build on canonical chromosomes (chr1..22,X,Y,MT)\n",
    "        Returns path to index_dir used.\n",
    "        \"\"\"\n",
    "        from pathlib import Path\n",
    "        import shutil, subprocess, sys, os\n",
    "        index_dir = Path(index_dir)\n",
    "        index_dir.mkdir(parents=True, exist_ok=True)\n",
    "        max_threads = int(max_threads or min(self.total_threads, 8))  # cap default threads to 8 for safety\n",
    "    \n",
    "        # small helper to call STAR and capture return\n",
    "        def _run_star(cmd, timeout=7200):\n",
    "            try:\n",
    "                print(\"Running:\", \" \".join(cmd))\n",
    "                subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=timeout)\n",
    "                return 0, None\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                rc = e.returncode\n",
    "                stderr = e.stderr or e.stdout\n",
    "                print(f\"STAR failed (rc={rc}). stderr snippet:\\n\", (stderr or \"\")[:1000])\n",
    "                return rc, stderr\n",
    "            except Exception as e:\n",
    "                print(\"Exception running STAR:\", e)\n",
    "                return -1, str(e)\n",
    "    \n",
    "        # If index already exists skip\n",
    "        if (index_dir / \"genomeParameters.txt\").exists():\n",
    "            print(\"STAR index already exists at\", index_dir)\n",
    "            return str(index_dir)\n",
    "    \n",
    "        print(\"Attempt 1: build STAR index with threads =\", max_threads)\n",
    "        cmd1 = [\n",
    "            self.star_path, \"--runMode\", \"genomeGenerate\",\n",
    "            \"--genomeDir\", str(index_dir),\n",
    "            \"--genomeFastaFiles\", str(genome_fasta),\n",
    "            \"--sjdbGTFfile\", str(gtf_file),\n",
    "            \"--sjdbOverhang\", \"100\",\n",
    "            \"--runThreadN\", str(max_threads)\n",
    "        ]\n",
    "        rc, stderr = _run_star(cmd1)\n",
    "        if rc == 0:\n",
    "            print(\"STAR index built successfully (attempt 1).\")\n",
    "            return str(index_dir)\n",
    "    \n",
    "        # If rc 137 or similar, try lowering memory/threads\n",
    "        print(\"\\nAttempt 2: retry with lowered threads and limitGenomeGenerateRAM\")\n",
    "        lower_threads = max(4, int(max_threads/2))\n",
    "        # try with 40GB\n",
    "        limit_ram = 40_000_000_000\n",
    "        cmd2 = cmd1 + [\"--runThreadN\", str(lower_threads), \"--limitGenomeGenerateRAM\", str(limit_ram)]\n",
    "        rc2, stderr2 = _run_star(cmd2)\n",
    "        if rc2 == 0:\n",
    "            print(\"STAR index built successfully (attempt 2).\")\n",
    "            return str(index_dir)\n",
    "    \n",
    "        # Attempt 3: build index on canonical chromosomes only (smaller FASTA/GTF)\n",
    "        print(\"\\nAttempt 3: build index on canonical chromosomes (chr1..22,X,Y,MT).\")\n",
    "        # prepare reduced fasta and gtf\n",
    "        work = index_dir.parent / \"reduced_reference\"\n",
    "        work.mkdir(parents=True, exist_ok=True)\n",
    "        reduced_fa = work / (Path(genome_fasta).stem + \".canonical.fa\")\n",
    "        reduced_gtf = work / (Path(gtf_file).stem + \".canonical.gtf\")\n",
    "    \n",
    "        # create canonical chromosome list - adjust names if your FASTA uses no 'chr' prefix\n",
    "        canonical = [\"chr\"+str(i) for i in range(1,23)] + [\"chrX\",\"chrY\",\"chrM\"]\n",
    "        # try both with and without 'chr' if not found\n",
    "        print(\"Creating canonical FASTA/GTF; this may take a moment.\")\n",
    "        try:\n",
    "            # attempt extraction using samtools faidx if available\n",
    "            if shutil.which(\"samtools\"):\n",
    "                idx = str(genome_fasta) + \".fai\"\n",
    "                if not Path(idx).exists():\n",
    "                    print(\"Indexing FASTA with samtools faidx...\")\n",
    "                    subprocess.run([\"samtools\", \"faidx\", str(genome_fasta)], check=True)\n",
    "                # determine actual contig names in FASTA\n",
    "                contigs = []\n",
    "                with open(str(genome_fasta)+\".fai\") as f:\n",
    "                    contigs = [l.split('\\t',1)[0] for l in f]\n",
    "                # intersect canonical with actual contigs (try both styles)\n",
    "                use_contigs = [c for c in canonical if c in contigs]\n",
    "                if not use_contigs:\n",
    "                    # try without 'chr' prefix\n",
    "                    canonical2 = [c.replace(\"chr\",\"\") for c in canonical]\n",
    "                    use_contigs = [c for c in canonical2 if c in contigs]\n",
    "                if not use_contigs:\n",
    "                    print(\"Could not find canonical contigs in FASTA index; aborting reduced-reference approach.\")\n",
    "                else:\n",
    "                    # write a temporary list file and use samtools faidx to extract\n",
    "                    listf = work / \"canonical_list.txt\"\n",
    "                    listf.write_text(\"\\n\".join(use_contigs))\n",
    "                    print(\"Extracting contigs:\", use_contigs[:10], \"... (total {})\".format(len(use_contigs)))\n",
    "                    subprocess.run([\"samtools\", \"faidx\", str(genome_fasta), \"-r\", str(listf), \"-o\", str(reduced_fa)], check=True)\n",
    "                    # filter GTF by contigs\n",
    "                    with open(reduced_gtf, \"w\") as out:\n",
    "                        with open(gtf_file) as g:\n",
    "                            for line in g:\n",
    "                                if line.startswith(\"#\"):\n",
    "                                    out.write(line)\n",
    "                                    continue\n",
    "                                parts = line.split(\"\\t\")\n",
    "                                if parts[0] in use_contigs:\n",
    "                                    out.write(line)\n",
    "            else:\n",
    "                print(\"samtools not found; cannot auto-extract canonical contigs. Please create reduced FASTA/GTf manually.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error creating reduced reference:\", e)\n",
    "    \n",
    "        # if reduced files created, run STAR on them\n",
    "        if reduced_fa.exists() and reduced_gtf.exists():\n",
    "            reduced_index = work / \"star_index_canonical\"\n",
    "            reduced_index.mkdir(parents=True, exist_ok=True)\n",
    "            cmd3 = [\n",
    "                self.star_path, \"--runMode\", \"genomeGenerate\",\n",
    "                \"--genomeDir\", str(reduced_index),\n",
    "                \"--genomeFastaFiles\", str(reduced_fa),\n",
    "                \"--sjdbGTFfile\", str(reduced_gtf),\n",
    "                \"--sjdbOverhang\", \"100\",\n",
    "                \"--runThreadN\", \"4\",\n",
    "                \"--limitGenomeGenerateRAM\", str(30_000_000_000)  # 30GB\n",
    "            ]\n",
    "            rc3, stderr3 = _run_star(cmd3)\n",
    "            if rc3 == 0:\n",
    "                print(\"STAR index built successfully on reduced reference:\", reduced_index)\n",
    "                return str(reduced_index)\n",
    "            else:\n",
    "                print(\"Reduced-reference STAR build also failed (rc={}).\".format(rc3))\n",
    "        else:\n",
    "            print(\"Reduced FASTA/GTF were not created; cannot attempt reduced index build.\")\n",
    "    \n",
    "        # If we reach here, all attempts failed\n",
    "        raise RuntimeError(\"STAR genomeGenerate failed after multiple attempts. \"\n",
    "                           \"Consider running on a machine with more RAM (>=64GB), \"\n",
    "                           \"or using a prebuilt STAR index.\")\n",
    "\n",
    "    \n",
    "    def align_reads(self, fastq1, fastq2, star_index, output_prefix, sample_name):\n",
    "        \"\"\"Align reads using STAR\"\"\"\n",
    "        \n",
    "        print(f\"Aligning reads for {sample_name}...\")\n",
    "        \n",
    "        star_cmd = [\n",
    "            self.star_path,\n",
    "            \"--genomeDir\", star_index,\n",
    "            \"--readFilesIn\", str(fastq1), str(fastq2),\n",
    "            \"--readFilesCommand\", \"zcat\",  # For .gz files\n",
    "            \"--outFileNamePrefix\", f\"{output_prefix}_\",\n",
    "            \"--outSAMtype\", \"BAM\", \"SortedByCoordinate\",\n",
    "            \"--outSAMunmapped\", \"Within\",\n",
    "            \"--quantMode\", \"GeneCounts\",  # Get gene counts directly\n",
    "            \"--runThreadN\", str(self.total_threads)\n",
    "        ]\n",
    "        \n",
    "        self.run_command(star_cmd, timeout=3600)\n",
    "        \n",
    "        # Output files\n",
    "        bam_file = f\"{output_prefix}_Aligned.sortedByCoord.out.bam\"\n",
    "        counts_file = f\"{output_prefix}_ReadsPerGene.out.tab\"\n",
    "        log_file = f\"{output_prefix}_Log.final.out\"\n",
    "        \n",
    "        return bam_file, counts_file, log_file\n",
    "    \n",
    "    def index_bam(self, bam_file):\n",
    "        \"\"\"Index BAM file\"\"\"\n",
    "        print(f\"Indexing BAM file: {bam_file}\")\n",
    "        \n",
    "        index_cmd = [self.samtools_path, \"index\", bam_file]\n",
    "        self.run_command(index_cmd, timeout=600)\n",
    "        \n",
    "        return f\"{bam_file}.bai\"\n",
    "\n",
    "def analyze_aml_rnaseq(fastq1, fastq2, output_dir, sample_name, \n",
    "                      reference_dir=None, skip_reference=False):\n",
    "    \"\"\"\n",
    "    Complete RNA-seq analysis pipeline for AML data\n",
    "    \"\"\"\n",
    "    \n",
    "    pipeline = RNASeqPipeline()\n",
    "    \n",
    "    # Check tools\n",
    "    if not pipeline.check_tools_installed():\n",
    "        print(\"Please install missing tools before proceeding\")\n",
    "        return None\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    qc_dir = output_dir / \"quality_control\"\n",
    "    alignment_dir = output_dir / \"alignments\"\n",
    "    reference_dir = Path(reference_dir) if reference_dir else output_dir / \"reference\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Quality control\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 1: Quality Control\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        pipeline.quality_control(fastq1, fastq2, qc_dir)\n",
    "        pipeline.basic_read_stats(fastq1, fastq2)\n",
    "        \n",
    "        if skip_reference:\n",
    "            print(\"\\nSkipping reference genome setup (skip_reference=True)\")\n",
    "            print(\"Make sure you have:\")\n",
    "            print(\"1. Reference genome FASTA file\")\n",
    "            print(\"2. Gene annotation GTF file\") \n",
    "            print(\"3. Pre-built STAR index\")\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Reference genome setup\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 2: Reference Genome Setup\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        genome_fa, gtf_file = pipeline.download_reference_genome(reference_dir)\n",
    "        \n",
    "        # Step 3: Build STAR index\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 3: Building STAR Index\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        star_index = pipeline.build_star_index(\n",
    "            genome_fa, gtf_file, reference_dir / \"star_index\"\n",
    "        )\n",
    "        \n",
    "        # Step 4: Alignment\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 4: Read Alignment\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        alignment_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_prefix = alignment_dir / sample_name\n",
    "        \n",
    "        bam_file, counts_file, log_file = pipeline.align_reads(\n",
    "            fastq1, fastq2, star_index, str(output_prefix), sample_name\n",
    "        )\n",
    "        \n",
    "        # Step 5: Index BAM\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 5: Indexing BAM file\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        bam_index = pipeline.index_bam(bam_file)\n",
    "        \n",
    "        # Step 6: Parse results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 6: Results Summary\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        results = {\n",
    "            \"sample_name\": sample_name,\n",
    "            \"bam_file\": bam_file,\n",
    "            \"bam_index\": bam_index,\n",
    "            \"counts_file\": counts_file,\n",
    "            \"log_file\": log_file,\n",
    "            \"qc_dir\": str(qc_dir),\n",
    "            \"reference_genome\": genome_fa,\n",
    "            \"annotation\": gtf_file,\n",
    "            \"star_index\": star_index\n",
    "        }\n",
    "        \n",
    "        # Parse gene counts\n",
    "        if Path(counts_file).exists():\n",
    "            counts_df = pd.read_csv(counts_file, sep='\\t', header=None, skiprows=4)\n",
    "            counts_df.columns = ['gene_id', 'unstranded', 'stranded_first', 'stranded_second']\n",
    "            \n",
    "            total_genes = len(counts_df)\n",
    "            expressed_genes = len(counts_df[counts_df['unstranded'] > 0])\n",
    "            total_reads = counts_df['unstranded'].sum()\n",
    "            \n",
    "            print(f\"Total genes: {total_genes:,}\")\n",
    "            print(f\"Expressed genes: {expressed_genes:,}\")\n",
    "            print(f\"Total aligned reads: {total_reads:,}\")\n",
    "            \n",
    "            results[\"gene_counts\"] = counts_df\n",
    "            results[\"total_genes\"] = total_genes\n",
    "            results[\"expressed_genes\"] = expressed_genes\n",
    "            results[\"total_reads\"] = total_reads\n",
    "        \n",
    "        print(f\"\\nRNA-seq analysis completed successfully!\")\n",
    "        print(f\"Results saved in: {output_dir}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ef5af2e-4685-4c18-a862-33c6bd59a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Pulling the data systematically from the mounted drive\n",
    "base_dir = \"/mnt/a/Projects/RA_Assignment\"\n",
    "\n",
    "patients = [\"Patient1\"]\n",
    "\n",
    "patient_files = {\"Patient1\": (\"SRR1918235_1.fastq.gz\", \"SRR1918235_2.fastq.gz\")}\n",
    "\n",
    "# Output dir for results\n",
    "results_dir = os.path.join(base_dir, \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c880a8",
   "metadata": {},
   "source": [
    "### Added: Visualization helpers\n",
    "\n",
    "This cell adds plotting and FastQC parsing helpers to `RNASeqPipeline`:\n",
    "- `parse_star_log`\n",
    "- `plot_star_metrics`\n",
    "- `plot_top_expressed_genes`\n",
    "- `plot_expression_distribution`\n",
    "- `plot_top_variable_genes_heatmap`\n",
    "- `parse_fastqc_summary`\n",
    "\n",
    "These are appended to the end of the notebook and monkeypatched onto the class if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9d590a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization helpers attached to RNASeqPipeline\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- ADDED: Visualization & FastQC parsing helpers for RNASeqPipeline ---\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_star_log(self, file):\n",
    "    \"\"\"Parse STAR Log.final.out into a dict of metric -> value.\n",
    "    STAR log lines look like: '                             Started job on |   Nov 13 15:43:11'\n",
    "    We split on the '|' character and strip whitespace.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if '|' in line:\n",
    "                parts = line.split('|')\n",
    "                if len(parts) == 2:\n",
    "                    key, val = [x.strip() for x in parts]\n",
    "                    metrics[key] = val\n",
    "    return metrics\n",
    "\n",
    "def plot_star_metrics(self, log_file, ax=None, figsize=(8,4), save=None):\n",
    "    \"\"\"Create a small bar chart / summary of important STAR metrics.\n",
    "    Returns the parsed metrics dict and matplotlib figure (ax).\n",
    "    \"\"\"\n",
    "    metrics = self.parse_star_log(log_file)\n",
    "    \n",
    "    # pick a standard subset of keys (if present)\n",
    "    keys_of_interest = [\n",
    "        'Number of input reads',\n",
    "        'Average input read length',\n",
    "        'Uniquely mapped reads number',\n",
    "        'Uniquely mapped reads %',\n",
    "        'Number of reads mapped to multiple loci',\n",
    "        '% of reads mapped to multiple loci',\n",
    "        'Number of reads mapped to too many loci',\n",
    "        '% of reads mapped to too many loci',\n",
    "        'Unmapped reads: too short',\n",
    "        'Unmapped reads: too many mismatches',\n",
    "        'Unmapped reads: other'\n",
    "    ]\n",
    "    present = [(k, metrics[k]) for k in keys_of_interest if k in metrics]\n",
    "    if not present:\n",
    "        # fallback: show a pretty-printed text summary if keys not found\n",
    "        fig, ax = plt.subplots(figsize=(8,6))\n",
    "        ax.axis('off')\n",
    "        txt = '\\n'.join(f\"{k}: {v}\" for k,v in metrics.items())\n",
    "        ax.text(0, 1, txt, va='top', fontsize=9, family='monospace')\n",
    "        if save:\n",
    "            fig.savefig(save, bbox_inches='tight', dpi=150)\n",
    "        return metrics, ax\n",
    "    \n",
    "    labels = [p[0] for p in present]\n",
    "    vals = []\n",
    "    for _, v in present:\n",
    "        # remove trailing % or commas; convert to float if possible\n",
    "        vv = v.replace('%','').replace(',','').strip()\n",
    "        try:\n",
    "            vals.append(float(vv))\n",
    "        except:\n",
    "            # keep as NaN so we can skip in plotting numeric bars\n",
    "            vals.append(math.nan)\n",
    "    \n",
    "    # pick numeric entries\n",
    "    numeric_labels = []\n",
    "    numeric_vals = []\n",
    "    for l, val in zip(labels, vals):\n",
    "        if not math.isnan(val):\n",
    "            numeric_labels.append(l)\n",
    "            numeric_vals.append(val)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    if numeric_vals:\n",
    "        ax.barh(range(len(numeric_vals)), numeric_vals, align='center')\n",
    "        ax.set_yticks(range(len(numeric_vals)))\n",
    "        ax.set_yticklabels(numeric_labels, fontsize=9)\n",
    "        ax.invert_yaxis()\n",
    "        ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f\"{x:g}\"))\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_title(\"STAR alignment metrics (selected)\")\n",
    "        plt.tight_layout()\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "        ax.text(0, 1, \"No numeric STAR metrics available to plot.\", va='top')\n",
    "    \n",
    "    if save:\n",
    "        fig.savefig(save, bbox_inches='tight', dpi=150)\n",
    "    return metrics, ax\n",
    "\n",
    "def plot_top_expressed_genes(self, counts_df, top_n=20, ax=None, figsize=(8,6), save=None):\n",
    "    \"\"\"\n",
    "    counts_df expected format: columns like ['gene_id', 'unstranded', 'stranded_first', ...]\n",
    "    Plot top_n genes by unstranded counts.\n",
    "    \"\"\"\n",
    "    # defensive checks\n",
    "    if 'unstranded' not in counts_df.columns:\n",
    "        raise ValueError(\"counts_df must contain 'unstranded' column produced by STAR (--quantMode GeneCounts)\")\n",
    "    \n",
    "    top = counts_df.sort_values('unstranded', ascending=False).head(top_n)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.barh(range(len(top)), top['unstranded'].values[::-1], align='center')\n",
    "    ax.set_yticks(range(len(top)))\n",
    "    ax.set_yticklabels(top['gene_id'].values[::-1], fontsize=9)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel(\"Read counts (unstranded)\")\n",
    "    ax.set_title(f\"Top {top_n} expressed genes\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        fig.savefig(save, bbox_inches='tight', dpi=150)\n",
    "    return fig, ax\n",
    "\n",
    "def plot_expression_distribution(self, counts_df, ax=None, figsize=(6,4), save=None):\n",
    "    \"\"\"\n",
    "    Shows histogram of counts per gene (log10 scale).\n",
    "    \"\"\"\n",
    "    if 'unstranded' not in counts_df.columns:\n",
    "        raise ValueError(\"counts_df must contain 'unstranded' column\")\n",
    "    \n",
    "    counts = counts_df['unstranded'].values\n",
    "    counts_pos = counts[counts > 0]\n",
    "    if len(counts_pos) == 0:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax.text(0.5,0.5,\"No positive counts to plot\", ha='center')\n",
    "        return fig, ax\n",
    "    \n",
    "    log_counts = np.log10(counts_pos)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.hist(log_counts, bins=50)\n",
    "    ax.set_xlabel(\"log10(read counts)\")\n",
    "    ax.set_ylabel(\"Number of genes\")\n",
    "    ax.set_title(\"Distribution of gene expression (log10 counts)\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        fig.savefig(save, bbox_inches='tight', dpi=150)\n",
    "    return fig, ax\n",
    "\n",
    "def plot_top_variable_genes_heatmap(self, counts_df, top_n=50, figsize=(8,8), save=None):\n",
    "    \"\"\"\n",
    "    Create a small heatmap of top N most variable genes across columns (if multiple samples).\n",
    "    If counts_df has only one sample (single column), this will just plot the top N values as a column heatmap.\n",
    "    \"\"\"\n",
    "    # Accept both the STAR counts format and standard gene x sample countframes.\n",
    "    df = counts_df.copy()\n",
    "    if 'unstranded' in df.columns and df.shape[1] == 4:\n",
    "        # STAR output single-sample: convert to gene x 1 sample\n",
    "        expr = pd.DataFrame(df['unstranded'].values, index=df['gene_id'], columns=['sample'])\n",
    "    else:\n",
    "        # assume gene_id is index or first column\n",
    "        if 'gene_id' in df.columns:\n",
    "            expr = df.set_index('gene_id').select_dtypes(include=[np.number])\n",
    "        else:\n",
    "            expr = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    if expr.shape[1] == 0:\n",
    "        raise ValueError(\"No numeric expression columns found to plot heatmap\")\n",
    "    \n",
    "    # compute variability and pick top N\n",
    "    var = expr.var(axis=1).sort_values(ascending=False).head(top_n)\n",
    "    top_expr = expr.loc[var.index]\n",
    "    \n",
    "    # simple plotting using imshow\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    im = ax.imshow(np.log10(top_expr + 1), aspect='auto', interpolation='nearest')\n",
    "    ax.set_yticks(range(len(top_expr.index)))\n",
    "    ax.set_yticklabels(top_expr.index, fontsize=8)\n",
    "    ax.set_xticks(range(len(top_expr.columns)))\n",
    "    ax.set_xticklabels(top_expr.columns, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_title(f\"Heatmap of top {top_n} variable genes (log10(count+1))\")\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label(\"log10(count+1)\")\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        fig.savefig(save, bbox_inches='tight', dpi=150)\n",
    "    return fig, ax\n",
    "\n",
    "def parse_fastqc_summary(self, fastqc_zip_or_html):\n",
    "    \"\"\"Attempt to parse FastQC output (if you produced JSON or can parse HTML).\n",
    "    This is a light-weight helper that looks for the FastQC summary.txt in a directory or zip.\n",
    "    Returns a dict of module -> status.\n",
    "    \"\"\"\n",
    "    p = Path(fastqc_zip_or_html)\n",
    "    summary = {}\n",
    "    try:\n",
    "        if p.is_dir():\n",
    "            summary_file = p / \"summary.txt\"\n",
    "            if summary_file.exists():\n",
    "                for line in open(summary_file):\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 2:\n",
    "                        status, module = parts[0], parts[1]\n",
    "                        summary[module] = status\n",
    "        else:\n",
    "            # try to find embedded 'FastQC Report' title in html for a quick status\n",
    "            txt = p.read_text(encoding='utf-8', errors='ignore')\n",
    "            if \"FastQC Report\" in txt:\n",
    "                summary['report'] = \"OK (html detected)\"\n",
    "    except Exception as e:\n",
    "        summary['error'] = str(e)\n",
    "    return summary\n",
    "\n",
    "# Monkeypatch onto RNASeqPipeline class if it exists in the user's environment\n",
    "try:\n",
    "    RNASeqPipeline.parse_star_log = parse_star_log\n",
    "    RNASeqPipeline.plot_star_metrics = plot_star_metrics\n",
    "    RNASeqPipeline.plot_top_expressed_genes = plot_top_expressed_genes\n",
    "    RNASeqPipeline.plot_expression_distribution = plot_expression_distribution\n",
    "    RNASeqPipeline.plot_top_variable_genes_heatmap = plot_top_variable_genes_heatmap\n",
    "    RNASeqPipeline.parse_fastqc_summary = parse_fastqc_summary\n",
    "    print(\"Visualization helpers attached to RNASeqPipeline\")\n",
    "except Exception as e:\n",
    "    print(\"Could not attach helpers automatically:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82d5f3b-6d3d-4d2e-89fd-b594598a83b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AML RNA-seq Analysis Pipeline\n",
      "========================================\n",
      "This dataset contains:\n",
      "- Tissue: Bone marrow\n",
      "- Cell Type: Acute myeloid leukemia (AML) cells\n",
      "- Data Type: RNA-seq (gene expression)\n",
      "- Organism: Homo sapiens\n",
      "\n",
      "This pipeline will:\n",
      "1. Run quality control (FastQC)\n",
      "2. Download human reference genome\n",
      "3. Build STAR alignment index\n",
      "4. Align reads and count genes\n",
      "5. Generate expression analysis\n",
      "\n",
      "RESOURCE REQUIREMENTS:\n",
      "- Time: 2-4 hours for full pipeline\n",
      "- Memory: 30+ GB RAM for STAR index\n",
      "- Storage: 50+ GB for reference genome and results\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Proceed with full analysis? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ STAR aligner found: /home/atharva/miniforge/envs/rna_env/bin/STAR\n",
      "✓ SAMtools found: /home/atharva/miniforge/envs/rna_env/bin/samtools\n",
      "✓ featureCounts (subread) found: /home/atharva/miniforge/envs/rna_env/bin/featureCounts\n",
      "✓ FastQC found: /home/atharva/miniforge/envs/rna_env/bin/fastqc\n",
      "✓ SeqKit found: /home/atharva/miniforge/envs/rna_env/bin/seqkit\n",
      "\n",
      "==================================================\n",
      "STEP 1: Quality Control\n",
      "==================================================\n",
      "Running FastQC (non-interactive)...\n",
      "\n",
      "$ /home/atharva/miniforge/envs/rna_env/bin/fastqc -o /mnt/a/Projects/RA_Assignment/rnaseq_results/Patient1/quality_control -t 4 /mnt/a/Projects/RA_Assignment/Patient1/SRR1918235_1.fastq.gz /mnt/a/Projects/RA_Assignment/Patient1/SRR1918235_2.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "# Usage for your AML data\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    fastq1 = Path(base_dir) / \"Patient1\" / \"SRR1918235_1.fastq.gz\"\n",
    "    fastq2 = Path(base_dir) / \"Patient1\" / \"SRR1918235_2.fastq.gz\"\n",
    "    output_dir = Path(base_dir) / \"rnaseq_results\" / \"Patient1\"\n",
    "    patient = \"Patient1_AML\"\n",
    "    \n",
    "    print(\"AML RNA-seq Analysis Pipeline\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"This dataset contains:\")\n",
    "    print(\"- Tissue: Bone marrow\")\n",
    "    print(\"- Cell Type: Acute myeloid leukemia (AML) cells\") \n",
    "    print(\"- Data Type: RNA-seq (gene expression)\")\n",
    "    print(\"- Organism: Homo sapiens\")\n",
    "    print()\n",
    "    \n",
    "    print(\"This pipeline will:\")\n",
    "    print(\"1. Run quality control (FastQC)\")\n",
    "    print(\"2. Download human reference genome\")\n",
    "    print(\"3. Build STAR alignment index\")\n",
    "    print(\"4. Align reads and count genes\")\n",
    "    print(\"5. Generate expression analysis\")\n",
    "    print()\n",
    "    \n",
    "    # Warning about resource requirements\n",
    "    print(\"RESOURCE REQUIREMENTS:\")\n",
    "    print(\"- Time: 2-4 hours for full pipeline\")\n",
    "    print(\"- Memory: 30+ GB RAM for STAR index\")\n",
    "    print(\"- Storage: 50+ GB for reference genome and results\")\n",
    "    print()\n",
    "    \n",
    "    proceed = input(\"Proceed with full analysis? (y/n): \").lower().strip()\n",
    "    \n",
    "    if proceed == 'y':\n",
    "        try:\n",
    "            results = analyze_aml_rnaseq(\n",
    "                fastq1=fastq1,\n",
    "                fastq2=fastq2,\n",
    "                output_dir=output_dir,\n",
    "                sample_name=patient\n",
    "            )\n",
    "            \n",
    "            if results:\n",
    "                print(f\"\\nAnalysis completed!\")\n",
    "                print(f\"Key results:\")\n",
    "                print(f\"- BAM file: {results['bam_file']}\")\n",
    "                print(f\"- Gene counts: {results['counts_file']}\")\n",
    "                print(f\"- Expressed genes: {results.get('expressed_genes', 'N/A')}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Analysis failed: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Analysis cancelled.\")\n",
    "        print(\"\\nTo run just quality control:\")\n",
    "        print(\"pipeline = RNASeqPipeline()\")\n",
    "        print(\"pipeline.quality_control(fastq1, fastq2, 'qc_output')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b6628-33a2-4aa4-9a58-d504e97ab93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame, display, HTML\n",
    "reports = results.get(\"fastqc_reports\")\n",
    "if reports:\n",
    "    for html in reports[\"htmls\"]:\n",
    "        print(\"Embedding:\", html)\n",
    "        display(IFrame(src=html, width=1000, height=600))\n",
    "else:\n",
    "    print(\"No FastQC HTML reports found in results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b37ac-f77e-4fe6-bed9-cf4a9d1fa6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
